{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f04061f",
   "metadata": {},
   "source": [
    "<!--"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3854acad-dc94-4dd7-b65e-e336e57f2428",
   "metadata": {
    "citation-manager": {
     "citations": {
      "6d9q5": [
       {
        "id": "13409951/AYIUUYSI",
        "source": "zotero"
       }
      ],
      "6pejh": [
       {
        "id": "13409951/K57UGHVY",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "# Final Report - Work in Progress\n",
    "- Research Hypothesis / Questions:\n",
    "    - Is Formula 1 fandom Toxic?\n",
    "    - Are there specific groups that show more toxic behaviour then others?\n",
    "    - Is the toxicity a \"self-made\" problem of Formula 1?\n",
    "- APIs: Youtube\n",
    "    - (Not reddit as post are often off topic especially during the off season, that we are currently in)\n",
    "- Methods:\n",
    "    - TBD\n",
    "    - Dictionary\n",
    "        - Formula 1 specific words that are toxic\n",
    "        - racism / ethnic slurs -> [@ethnic_slurs]\n",
    "        - toxicity -> [@orthrus-lexicon_orthrus_2022]\n",
    "        - hate speech -> [@van_der_vegt_grievance_2021]\n",
    "        - insults -> [@van_der_vegt_grievance_2021]\n",
    "    - Transformer classifier\n",
    "        - sentiment -> cardiffnlp/twitter-roberta-base-sentiment-latest [@tweet_sentiment_classifier]\n",
    "        - hate speech -> Hate-speech-CNERG/dehatebert-mono-english [@racism_classifier]\n",
    "    - statistical analysis\n",
    "        - group toxic behavior by drivers and teams\n",
    "        - group by topics\n",
    "            - topic modelling?\n",
    "- Contents:\n",
    "    - Introduction\n",
    "        - What is Formula 1\n",
    "        - Why do we need to analyze this\n",
    "        - introduce the three research questions / hypothesis\n",
    "    - Fundamentals\n",
    "        - Formula 1\n",
    "        - What is fandom\n",
    "          - \n",
    "        - Defining toxic fan behavior\n",
    "        - Youtube API\n",
    "        - Maybe explaining the used methods?\n",
    "    - Concept\n",
    "        - What will be done\n",
    "        - How will i be doing it\n",
    "    - Creating the Dataset\n",
    "        - Explain Dataset creation\n",
    "    - Applying Method 1\n",
    "    - Applying Method 2\n",
    "    - Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0175e8b5",
   "metadata": {},
   "source": [
    "-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db77bb04",
   "metadata": {},
   "source": [
    "# Analysing Toxicity in Formula 1 Fandom - Computational Analysis of Communications Final\n",
    "Author: Leon Knorr\n",
    "\n",
    "Matr-Nr: 1902854"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d9e388",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "In order to use Citations in Jupyter Notebook, the whole Notebook has to be converted to markdown and after that, the markdown file has to be compiled with LATEX and the bibliography and bibliography style is injected. Because of that Citations and the bibliography are only visible in the PDF version of the notebook. However because comments contain emojis, and other special characters, the output of each code cell has to be cleared before the notebook is converted otherwise the pdf compile will fail. In addition to that the formating of the code cells in the pdf document is not necessarily perfect. As a result, Citations and bibliography will only be correctly visible in the PDF version, where as code and its output is only visible in the notebook source."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a5bd92",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Formula 1 is the highest class of international racing for open-wheel single-seater formula racing cars and is generally considered the most competitive, fastest and hardest class of motor racing. Since it’s first season in 1950, Formula 1 is visiting a diverse list of many different countries, where the best drivers in the world are racing against each other in teams of two drivers to determine the best driver and the best team on the Formula 1 grid [@about_f1]. These events are visited by thousands of Fans, with millions more following them on television and social media. With the 2021 season being one of the closest and most entertaining seasons in the history of Formula 1, where Red Bulls Max Verstappen beat Mercedes driver Lewis Hamilton in the grand finale of the season under controversial circumstances after a full season of controversy, drama and intense on track battles and with the release of Netflix Drive To Survive, Formula 1s popularity is growing rapidly. But, reports of Toxic and abusive Fan behavior at events and in comment sections on social media are accumulating, and casts an ugly shadow over Formula 1s latest successes [@woodhouse_scary_2022].\n",
    "As the reports over toxic and abusive fan behaviours in social media and at live events are rising, Formula 1 as well as Fans and drivers are taking a stand against toxicity in the Formula 1 community. However, an independent and scientific analysis of this topic is missing and therefore the accusations are sort of hanging in the air without a solid scientific foundation. Therefore, in order to tackle this problem research into the toxicity of Formula 1 fandom is a necassety to gain valuable insights into understanding the problem, where it originates from and to build a foundation for future measures to make attending Formula 1 events as well as the media around it a safer and more enjoyable experience. To take the first step into this direction, this thesis will analyse Youtube comments of the Formula 1 channel in order to determine:\n",
    "\n",
    "- If the Formula 1 fandom is toxic\n",
    "- Are there specific groups that are more toxic then others?\n",
    "- Is the toxicity a \"self-made\" problem of Formula 1 and where is the toxicity originating from?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edfdb523",
   "metadata": {},
   "source": [
    "## Fundamentals\n",
    "In this chapter the necessary fundamental knowledge is presented.\n",
    "\n",
    "### Formula 1\n",
    "Formula 1 is the worlds most prestigous motor racing competition, as well as the world's most popular annual sporting series [@about_f1]. It marks the highest class of international open-wheel single-seater formula racing. The first Formula 1 competition was held in 1950, since then the competiton for the world drivers championship (wdc) which determines the worlds best driver and the world constructors championship (wcc) which determines the best team, is held annualy and is sanctioned by the Fédération Internationale de l'Automobile (FIA). During the competition (also called a season), Formula 1 visits a variety of different countries and racing tracks, each event (Grands Prix) is attended by thousands of people with millions watching from home [@formula_1_2023]. All rights of the Formula 1 brand and the competition itself is owned by Formula One World Championship Limited, which is a corporation, that provides media distribution and promotion services, besides that, it controls the contracts, distribtution, and commercial management of rights and licenses of formula 1 [@formula_1_limited_company_profile]. The term Formula 1 is used to describe the corporation, as well as the competition, as they can't exist without each other.\n",
    "\n",
    "### What is Fandom\n",
    "According to Cornel Sandvoss Fandom is a community of people that are regularly, consuming a given popular narrative or text with great emotional involvement [@toxic_fandom]. The members of the community are called fans, which is a short form of \"fanatic\" [@arouh_toxic_2020]. In other words, a fandom is a community of people that are fanatic about a popular narrative or text such as a tv series, movie franchise or sports.\n",
    "\n",
    "Becoming a fan starts with the adoption of a fan identity about a fan object, thus fandom can be a powerful of defining the self. The fan object can be anything that people can be fanatic about, this may be a simple object such as trains or a virtual asset such as a movie franchise. Therefore, by taking part in a fandom, people are expressing themselfs through an identity they've chosen for themselfs. As a result, fans may lead to see the fan object as an extension of themselfs and thus react personally threatened if the fan object is facing a threat such as accusations etc [@toxic_fandom]. In addition to creating a strong part of their own identity, fans feel more connected or socialised through their fandom, as studies indicate, that even if fans don't interact with other members of a fan community, they still perceive themselfs as part of that community. Because of that, fans not only become personally invested in their fandom, they become socially invested as well [@toxic_fandom].\n",
    "\n",
    "As a result of the strong connection fans build up to their fan object, the time-frame in which this self identity has been chosen is also playing a role. As an example, many people build a fandom in their childhood about a tv series, franchise or sport, this often leads to them feeling entitled to having their fan object preserved as they deem acceptable. This behaviour is also called fan entitlement. A good example for this behaviour are the news movies and series in the Lord of the Rings and Star Wars franchises, as most fan communities of these franchises have been outraged about the new characters and story lines, where many people claimed that this \"ruined their childhood\" [@toxic_fandom].\n",
    "\n",
    "From an economic point of view, fandom and fan cultures are seen as the ideal costumers. They are eager to get their hands on the newest products and they are stable with re-occuring purchases, since intense consumption is considered a part of the fan identity [@arouh_toxic_2020].\n",
    "\n",
    "### Defining Toxic Fan behaviour\n",
    "In the first place, toxic fandom is a buzzword, that is widely used throughout media to describe or identify fans who engage in behaviors that are considered negative or unaccaptable. This behavior can range from simple negative responses to bullying other members of a fandom or those involved in the creation of the fan object [@toxic_fandom]. Most of this behaviour can be observed online in social media, there are however reports of toxic behaviour in real-life as well, such as abusive behaviour at events.\n",
    "\n",
    "The word toxic itself however is defined as \"of relating to, or caused by a toxin,\" \"of the nature of a poison; poisonous\" [@arouh_toxic_2020]. This definition originally originates from medival latin, where it refers to poisoned arrows or to being imbued with poison. Following this definition, it is an *external* substance that is toxic and not a person or their behaviour. However in recent years the understanding of this definition has shifted, today someones actions or the emotions experienced or types of character are now understood as poisonous or \"toxic\" [@arouh_toxic_2020]. This definition is closely related to the definition of the word fan, as explained earlier, fan originates from fanatic, which is traditionally linked to madness and demonic posession. This traditional and long obselete link is often exploited by media outlets to mark fans as psychopaths whose frustrated fantasies of intimate relationships or unsatisfied desires with the fan object take violent and ant-social forms [@arouh_toxic_2020]. In order to maintain this hypothesis, media often picks the most miserable and negative or \"click-bait\" examples of fan behaviour, as it creates the most attention and keeps the viewing figures high [@arouh_toxic_2020], [@proctor_editors_2018]. These circumstances are additionally amplified by social media plattforms, as they promote toxic behaviour, because it usually creates a lot of interactions. Therefore, it is our overall understanding of what a fan is that marks a him as a toxic \"other\".\n",
    "\n",
    "What is also observed, is that \"toxic\" fans often fall back to racist and mysogenistic behaviour compared with hate speech in order to defend their fan object or view point. This often comes with a feeling of \"power loss\" for the \"toxic fan\". Because of that, current social-, ideological- and political conflicts are becoming more and more frequent as a topic in toxic behaviour [@proctor_editors_2018], [@arouh_toxic_2020], [@toxic_fandom]. For some members of the fan communities, this feeling of power loss is amplified by current political circumstances where they feel a feeling of disempowerment at their loss of priviliged status in society because of gender discussions or woman rights movements. Thus toxic fans are often painted as angry white, heterosexual men or members of the \"alt-right\" community. However in many cases, fan communities are used as a plattform to spread this hatered or ideological ideas because it creates a lot of attention in social networks as well as from the media. The media then progresses to paint fandom and online culture as more and more toxic because it creates \"maximum cultural penetration\" [@proctor_editors_2018]. This trend has led to the phenomenon of *progressive toxicity*, where other fans \"rush to prove one's moral superiority by speaking out against some racist, sexist or otherwise hurtful sentiment, the sentiment is often amplified on a scale that wouldn't have been possible had people not taken the bait\" [@proctor_editors_2018]. This rush to prove morally better than the toxic other often leads to toxic behavior by the defender itself. Because of that, toxic practices more and more frequently are instantiations of larger political or cultural polarizations and they depict the current socio-political climate. Thus toxic fan behaviour is often observed as a conlflict between the \"political correct\" pro-diversity crowd, which are also called social justice warriors (SJWs) and the members of the so-called \"alt-right\" hell-bent [@proctor_editors_2018].\n",
    "\n",
    "However toxic fan behaviour is not limited to racist, misogynistic comments that can also include hate-speech. Some toxic fan are even going as far as to writing death or rape threats, doxing people (doxing refers to leaking personal information online) or to show abusive and harassing behaviour in public against other groups [@proctor_editors_2018], [@arouh_toxic_2020]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b974c295",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43eecf97",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The dataset that will be used throughout this thesis consists of 40200 Comments with replys from 500 youtube videos that were uploaded since 2020 of the formula 1 youtube channel. To obtain this data, the Youtube API V3 was used.\n",
    "\n",
    "First up, the API has to be initialised, for this an api key is needed, that has to be stored in a .env file in the same directory as the jupyter notebook. This api key is then read in the following code cell and the youtube api is initialized through googles official googleapiclient library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40637624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_keys = dotenv_values(\"keys.env\")\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "api_key = api_keys[\"YOUTUBE_API_KEY\"]\n",
    "max_results = 1000\n",
    "youtube_api = googleapiclient.discovery.build(api_service_name, api_version, developerKey = api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c16df0b",
   "metadata": {},
   "source": [
    "Now request to the Youtube API V3 can be made. Before we can scrape comments, the video id of the video that comments want to be obtain from is needed. Therefore, data about all videos since 2020 until now are requested. However the api will only retrieve 50 items per request, if there are more items that fit the search query the response is paged and contains a *nextPageToken*, that can be used to obtain the next 50 items. Requesting all videos since 2020 allows the dataset to span a timeframe of three years and will allow to analyze toxicity over time as well and will also paint a broader picture of how the F1 fandom developed. After obtaining all video information, the video ids are extracted and safed into a list, which is used later to obtain the actual comment threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Formula1_official_channel = youtube_api.channels().list(part='snippet' ,forUsername='Formula1').execute()['items'][0]\n",
    "videos_after_2020 = youtube_api.search().list(channelId=Formula1_official_channel[\"id\"],\n",
    "        maxResults=max_results,\n",
    "        publishedAfter=\"2020-01-01T00:00:00Z\",\n",
    "        part='id').execute()\n",
    "video_ids_after_2020 = [item['id']['videoId'] for item in videos_after_2020['items']]\n",
    "while len(video_ids_after_2020) < max_results and \"nextPageToken\" in videos_after_2020.keys():\n",
    "        videos_after_2020 = youtube_api.search().list(channelId=Formula1_official_channel[\"id\"],\n",
    "        maxResults=max_results,\n",
    "        publishedAfter=\"2020-01-01T00:00:00Z\",\n",
    "        part='id',\n",
    "        pageToken=videos_after_2020[\"nextPageToken\"]).execute()\n",
    "        video_ids_after_2020 = video_ids_after_2020 + [item['id']['videoId'] for item in videos_after_2020['items']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a71dd7a1",
   "metadata": {},
   "source": [
    "Besides the list of video ids, the data is also parsed into a dataframe. This allows to take general video information such as like count, video title, the overall comment count etc. into consideration for the final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for video_id in video_ids_after_2020:\n",
    "    video_data = youtube_api.videos().list(part='snippet, statistics', id=video_id).execute()\n",
    "    snippet = video_data['items'][0]['snippet']\n",
    "    statistics = video_data['items'][0]['statistics']\n",
    "    df_list.append(\n",
    "    {\n",
    "        \"video_id\":video_id,\n",
    "        \"title\": snippet['title'],\n",
    "        \"description\": snippet['description'],\n",
    "        \"channel\": snippet['channelTitle'],\n",
    "        \"published_at\": snippet['publishedAt'],\n",
    "        \"tags\": snippet['tags'] if \"tags\" in snippet.keys() else None,\n",
    "        \"like_count\": statistics['likeCount'],\n",
    "        \"favorite_count\": statistics['favoriteCount'],\n",
    "        \"comment_count\": statistics['commentCount'] if \"commentCount\" in statistics.keys() else 0\n",
    "    })\n",
    "\n",
    "videos = pd.DataFrame(df_list)\n",
    "videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988c4e58",
   "metadata": {},
   "source": [
    "Now that all the necessary video information has been obtained, the actual comments and replys can be requested. In order to achieve this, for every video id that has been retrieved earlier, a list of 15 comment threads is requested. Every comment thread consists of a topcomment, that has a number of replys associated with it. Because of the maximum quota of 10000 request units per day, for each video only 15 comments can be obtained, as each comment request costs one unit, for all 500 videos for 15 commenthreads per video, a quota usage of 7500 applies. Now for each retrieved top comment a maximum of 10 replies are requested. The corresponding data, is then parsed into one large dataframe, that contains the comment text as well as administrative information like the video id as well as the comment id and further useful information like the number of likes a comment / reply has or the publishing date. This additional information allows to further reason about the amount of interaction the particular comment got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_comments = []\n",
    "for video_id in video_ids_after_2020:\n",
    "    if videos.loc[videos['video_id'] == video_id].comment_count.iloc[0] == 0:\n",
    "        continue\n",
    "    top_level_comments = youtube_api.commentThreads().list(part=\"snippet\",\n",
    "        maxResults=15,\n",
    "        order=\"relevance\",\n",
    "        videoId=video_id).execute()['items']\n",
    "    for top_level_comment in top_level_comments:\n",
    "        replies = youtube_api.comments().list(part=\"snippet\",\n",
    "            maxResults=10,\n",
    "            parentId=top_level_comment['snippet']['topLevelComment']['id']).execute()['items']\n",
    "        df_list_comments.append(\n",
    "        {\n",
    "            \"video_id\": video_id,\n",
    "            \"id\": top_level_comment['snippet']['topLevelComment']['id'],\n",
    "            \"text\": top_level_comment['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "            \"user\": top_level_comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "            \"like_count\": top_level_comment['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "            \"published_at\": top_level_comment['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "            \"reply_count\": top_level_comment['snippet']['totalReplyCount']\n",
    "        })\n",
    "        for reply in replies:\n",
    "            df_list_comments.append(\n",
    "            {\n",
    "                \"video_id\": video_id,\n",
    "                \"id\": reply['id'],\n",
    "                \"text\": reply['snippet']['textDisplay'],\n",
    "                \"user\": reply['snippet']['authorChannelId']['value'],\n",
    "                \"like_count\": reply['snippet']['likeCount'],\n",
    "                \"published_at\": reply['snippet']['publishedAt'],\n",
    "                \"reply_count\": 0\n",
    "            })\n",
    "\n",
    "comment_df: pd.DataFrame = pd.DataFrame(df_list_comments)\n",
    "comment_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9830cb35",
   "metadata": {},
   "source": [
    "Last but not least the dataset is saved into a \"pickle\" file, which allows efficient storage of dataframes. This is especially useful if the notebook has to be restarted because the dataset doesn't has to be build from scratch and no quota or api access is required to perform analysis on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.to_pickle(\"datasets/video_data.pkl\")\n",
    "comment_df.to_pickle(\"datasets/comment_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2ed2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos: pd.DataFrame = pd.read_pickle(\"datasets/video_data.pkl\")\n",
    "comment_df: pd.DataFrame = pd.read_pickle(\"datasets/comment_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba4dbb67",
   "metadata": {},
   "source": [
    "### Dataset limitations\n",
    "Because of the quoate limit google has set for the youtube api, the dataset is only depicting a small section of the actual circumstances in the Formula 1 fandom. For example, for one video, a maximum of $15*10 = 150$ comments will be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.comment_count = videos.comment_count.astype(int)\n",
    "videos.comment_count.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7613297e",
   "metadata": {},
   "source": [
    "However, on average a video has 1250 comments. Thus a lot of fan interaction will be missed and is not included in this dataset. In addition to that, the dataset only uses the Youtube API as a source, however Formula 1 fandom spans over multiple platforms, especially Twitter, Instagram and Reddit. Thus it is possible that depending on the plattform toxic user interactions may be more frequent as they are governed differently. Nevertheless the dataset spans over a total of 40200 comments that can be analysed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8a094b",
   "metadata": {},
   "source": [
    "## Dictionary Analysis\n",
    "\n",
    "### Othrus Lexicon for Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e76030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>reply_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg</td>\n",
       "      <td>THAT SUPER MAX AT THE END WAS FANTASTIC</td>\n",
       "      <td>UCmCBpDZeM9LbTeWbMPW1N2A</td>\n",
       "      <td>4626</td>\n",
       "      <td>2021-12-03T19:48:05Z</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD</td>\n",
       "      <td>@IIIlllIII I got cold</td>\n",
       "      <td>UCQtwk7iBtv0hTwedp13yDKA</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-02T08:40:19Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9</td>\n",
       "      <td>@Miz is Awesome oh yes.</td>\n",
       "      <td>UCuzQCuC86Pmhq9ixSff1z-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-01T18:23:44Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu</td>\n",
       "      <td>Yea that is a fact, but if i would write like ...</td>\n",
       "      <td>UCgFRK81ZtgryEHR6GAxbWvQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-28T15:47:42Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW</td>\n",
       "      <td>@Miz is Awesome didn’t age to well ehh?</td>\n",
       "      <td>UCADfAgrwLT4ZzpGkC8L-4gw</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-28T02:08:51Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40195</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgwZ6iauaq1cULpaUvR4AaABAg</td>\n",
       "      <td>How you managed Juncao is so amazing. That has...</td>\n",
       "      <td>UCmHLqIoiFsxOwsIEsqAVgzQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-16T11:01:58Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40196</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugxb7TesuwCHGhDxZRJ4AaABAg</td>\n",
       "      <td>Absolute brilliance from Seb on the radio as a...</td>\n",
       "      <td>UCalYkEFZn7f91L316msDu7Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-25T16:01:06Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40197</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugx4RZlmzo8UHnkajsB4AaABAg</td>\n",
       "      <td>I&amp;#39;ll come back from time to time to watch ...</td>\n",
       "      <td>UCYDZKuIlaaGAR-BOvs7W63Q</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T01:40:42Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40198</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgxbU_9pwAhxAIQkcRp4AaABAg</td>\n",
       "      <td>Going to miss Seb!</td>\n",
       "      <td>UCd4Bo6BxK0PHQnukaxeqnZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T21:19:20Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40199</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgyKB7N5-weM_DcpKbt4AaABAg</td>\n",
       "      <td>Ferrari Seb 2015-2018 nice but than how he fig...</td>\n",
       "      <td>UCylv5dmN-I2r7WxT2090rhg</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-19T20:23:04Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                                 id  \\\n",
       "0      FZjG3oft5rs                         UgznjAR9SoXoE0gpoK54AaABAg   \n",
       "1      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD   \n",
       "2      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9   \n",
       "3      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu   \n",
       "4      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW   \n",
       "...            ...                                                ...   \n",
       "40195  g5yQmp1ctXk                         UgwZ6iauaq1cULpaUvR4AaABAg   \n",
       "40196  g5yQmp1ctXk                         Ugxb7TesuwCHGhDxZRJ4AaABAg   \n",
       "40197  g5yQmp1ctXk                         Ugx4RZlmzo8UHnkajsB4AaABAg   \n",
       "40198  g5yQmp1ctXk                         UgxbU_9pwAhxAIQkcRp4AaABAg   \n",
       "40199  g5yQmp1ctXk                         UgyKB7N5-weM_DcpKbt4AaABAg   \n",
       "\n",
       "                                                    text  \\\n",
       "0                THAT SUPER MAX AT THE END WAS FANTASTIC   \n",
       "1                                  @IIIlllIII I got cold   \n",
       "2                                @Miz is Awesome oh yes.   \n",
       "3      Yea that is a fact, but if i would write like ...   \n",
       "4                @Miz is Awesome didn’t age to well ehh?   \n",
       "...                                                  ...   \n",
       "40195  How you managed Juncao is so amazing. That has...   \n",
       "40196  Absolute brilliance from Seb on the radio as a...   \n",
       "40197  I&#39;ll come back from time to time to watch ...   \n",
       "40198                                 Going to miss Seb!   \n",
       "40199  Ferrari Seb 2015-2018 nice but than how he fig...   \n",
       "\n",
       "                           user  like_count          published_at  reply_count  \n",
       "0      UCmCBpDZeM9LbTeWbMPW1N2A        4626  2021-12-03T19:48:05Z           48  \n",
       "1      UCQtwk7iBtv0hTwedp13yDKA           0  2022-08-02T08:40:19Z            0  \n",
       "2      UCuzQCuC86Pmhq9ixSff1z-A           0  2022-02-01T18:23:44Z            0  \n",
       "3      UCgFRK81ZtgryEHR6GAxbWvQ           1  2021-12-28T15:47:42Z            0  \n",
       "4      UCADfAgrwLT4ZzpGkC8L-4gw           0  2021-12-28T02:08:51Z            0  \n",
       "...                         ...         ...                   ...          ...  \n",
       "40195  UCmHLqIoiFsxOwsIEsqAVgzQ           0  2022-11-16T11:01:58Z            0  \n",
       "40196  UCalYkEFZn7f91L316msDu7Q           0  2022-11-25T16:01:06Z            0  \n",
       "40197  UCYDZKuIlaaGAR-BOvs7W63Q           1  2022-11-10T01:40:42Z            0  \n",
       "40198  UCd4Bo6BxK0PHQnukaxeqnZg           1  2022-11-10T21:19:20Z            0  \n",
       "40199  UCylv5dmN-I2r7WxT2090rhg           0  2022-12-19T20:23:04Z            0  \n",
       "\n",
       "[40200 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d12e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'willies',\n",
       " 'sh*t',\n",
       " 'ell',\n",
       " 'lusting',\n",
       " 'bloodsucking',\n",
       " 'coon',\n",
       " 'fucker',\n",
       " 'illegals',\n",
       " 'supremacists',\n",
       " 'gaytard',\n",
       " 'parasite',\n",
       " 'poontang',\n",
       " 'mothafuckings',\n",
       " 'cunnie',\n",
       " 'isis',\n",
       " 'fistfucking',\n",
       " 'murderer',\n",
       " 'a**',\n",
       " 'insecure',\n",
       " 'steals',\n",
       " 'liespropag',\n",
       " 'embarrassent',\n",
       " 'tit',\n",
       " 'shithead',\n",
       " 'cyberfucked',\n",
       " 'narrowminded',\n",
       " 'fg',\n",
       " 'shiting',\n",
       " 'felching',\n",
       " 'petulance',\n",
       " 'ejaculating',\n",
       " 'bastardo',\n",
       " 'stank',\n",
       " 'looooooosers',\n",
       " 'masterbat3',\n",
       " 'fuker',\n",
       " 'ejaculate',\n",
       " 'chumps',\n",
       " 'birther',\n",
       " 'leftists',\n",
       " 'tittie5',\n",
       " 'd*ckhead',\n",
       " 'shitfull',\n",
       " 'shiz',\n",
       " 'jagoff',\n",
       " 'shagging',\n",
       " 'stupider',\n",
       " 'stupdity',\n",
       " '5h1t',\n",
       " 'nut',\n",
       " 'predators',\n",
       " 'bollox',\n",
       " 'stinker',\n",
       " 'wop',\n",
       " 'fagbag',\n",
       " 'dammit',\n",
       " 'rapes',\n",
       " 'sack',\n",
       " 'God damn',\n",
       " 'fuckwitt',\n",
       " 'buttholes',\n",
       " 'homophobic',\n",
       " 'trumplodites',\n",
       " 'beatings',\n",
       " 'chicken',\n",
       " 'cyberfucker',\n",
       " 'choad',\n",
       " 'dumbfuck',\n",
       " 'pissed off',\n",
       " 'ecoradicals',\n",
       " 'shills',\n",
       " 'asshopper',\n",
       " 'sh1t',\n",
       " 'narcissistic',\n",
       " 'perpetrators',\n",
       " 'fistfuck',\n",
       " 'narcissism',\n",
       " 'sissy',\n",
       " 'idiot',\n",
       " 'kunilingus',\n",
       " 'tupid',\n",
       " 'omg',\n",
       " 'lunatics',\n",
       " 'clusterfuck',\n",
       " 'suckered',\n",
       " 'masterbation',\n",
       " 'hate-on',\n",
       " 'disgraceful',\n",
       " 'stooooopid',\n",
       " 'dolt',\n",
       " 'ignoramu',\n",
       " 'muthafecker',\n",
       " 'fukkin',\n",
       " 'shitbreath',\n",
       " 'cunnilingus',\n",
       " 'paf',\n",
       " 'i*d*i*o*t*s',\n",
       " 'shithole',\n",
       " 'fake',\n",
       " 'murder',\n",
       " 'insan',\n",
       " 'shove',\n",
       " 'assshole',\n",
       " 'repulsive',\n",
       " 'mad',\n",
       " 'executioners',\n",
       " 'b!tch',\n",
       " 'passive-aggresive',\n",
       " 'vacuous',\n",
       " 'unfaithful',\n",
       " 'fuk',\n",
       " 'teets',\n",
       " 'hypocrite',\n",
       " 'penetrated',\n",
       " 'p0rn',\n",
       " 'gaydo',\n",
       " 'bunglers',\n",
       " 'titsucking',\n",
       " 'muslimists',\n",
       " 'arrse',\n",
       " 'cocksucker',\n",
       " 'cheeto',\n",
       " \"moron's\",\n",
       " 'authoritarian',\n",
       " 'anticatholic',\n",
       " 'renob',\n",
       " 'dumbshit',\n",
       " 'crazies',\n",
       " 'jan',\n",
       " 'misogynist',\n",
       " 'frucking',\n",
       " 'fanatics',\n",
       " 'unpatriotic',\n",
       " 'dumshit',\n",
       " 'hypocrisy',\n",
       " 'nitwit',\n",
       " 'laughable',\n",
       " 'asscracker',\n",
       " 'robbed',\n",
       " 'b17ch',\n",
       " 'fuck',\n",
       " 'antireligious',\n",
       " 'punanny',\n",
       " 'thundercunt',\n",
       " 'twatwaffle',\n",
       " 'pigheaded',\n",
       " 'feck',\n",
       " 'clitface',\n",
       " 'illogic',\n",
       " 'blindness',\n",
       " 'dork',\n",
       " 'unethical',\n",
       " 'kooch',\n",
       " 'bang',\n",
       " 'thieving',\n",
       " 'impostor',\n",
       " 'hairy',\n",
       " 'schmucks',\n",
       " 'vanity',\n",
       " 'whacko',\n",
       " 'foolishness',\n",
       " 'sexists',\n",
       " 'schlong',\n",
       " 'criminalmentally',\n",
       " 'blowjobs',\n",
       " 'unintelligent',\n",
       " 'fistfuckings',\n",
       " 'whoar',\n",
       " 'kook',\n",
       " 'clown-beast',\n",
       " 'crazy',\n",
       " 'pisser',\n",
       " 'jackoff',\n",
       " 'stupidiest',\n",
       " 'f--king',\n",
       " 'trumpean',\n",
       " 'kock',\n",
       " 'sickos',\n",
       " 'pussi',\n",
       " 'hellholes',\n",
       " 'fuckhole',\n",
       " 'shoots',\n",
       " 'cockwaffle',\n",
       " 'unsmart',\n",
       " 'carpetmuncher',\n",
       " 'darned',\n",
       " 'pussygrabbing',\n",
       " 'booooobs',\n",
       " 'gooch',\n",
       " 'molesters',\n",
       " 'clumsy',\n",
       " 'charlatans',\n",
       " 'bumbling',\n",
       " 'religioushypocrite',\n",
       " 'fuckup',\n",
       " 'spout',\n",
       " 'fuckboy',\n",
       " 'potheads',\n",
       " 'porchmonkey',\n",
       " 'bellend',\n",
       " 'islamists',\n",
       " 'stabs',\n",
       " 'genitals',\n",
       " 'unapologetic',\n",
       " 'hoer',\n",
       " 'monster',\n",
       " 'ass-jabber',\n",
       " 'jism',\n",
       " 'daft',\n",
       " 'jack***',\n",
       " 'vengeful',\n",
       " 'muffed',\n",
       " 'boob',\n",
       " 'puking',\n",
       " 'minder',\n",
       " 'offender',\n",
       " 'mo-fo',\n",
       " 'asshead',\n",
       " 'mutherfucker',\n",
       " 'dago',\n",
       " 'narcisist',\n",
       " 'f*uck',\n",
       " 'vulva',\n",
       " 'misogynic',\n",
       " 'dick-sneeze',\n",
       " 'bigot',\n",
       " 'fucktard',\n",
       " 'disgusted',\n",
       " 'fucking',\n",
       " 'robs',\n",
       " 'obnoxious',\n",
       " 'insidiously',\n",
       " 'pinocchio',\n",
       " 'prostitute',\n",
       " 'useless',\n",
       " 'testicles',\n",
       " 'xrated',\n",
       " 'panooch',\n",
       " 'janusfaced',\n",
       " 'whackjob',\n",
       " 'cunts',\n",
       " 'twofaced',\n",
       " 'self-loving',\n",
       " 'ar5e',\n",
       " 'punk',\n",
       " 'ass�',\n",
       " 'weasel',\n",
       " 'fannyfucker',\n",
       " 'muff',\n",
       " 'cipa',\n",
       " 'cockmunch',\n",
       " 'monsters',\n",
       " 'pussylicking',\n",
       " 'd1ck',\n",
       " 'master-bate',\n",
       " 'incurables',\n",
       " 'w00se',\n",
       " 'miserable',\n",
       " 'beastial',\n",
       " 'fails',\n",
       " 'cuntass',\n",
       " 'stone-stupid',\n",
       " 'boooobs',\n",
       " 'suckass',\n",
       " 'foul-mouthed',\n",
       " 'piglets',\n",
       " 'wackos',\n",
       " 'unwelcome',\n",
       " 'stupida',\n",
       " 'lapdog',\n",
       " 'donkey',\n",
       " 'treasonous',\n",
       " 'panties',\n",
       " 'hogs',\n",
       " 'bumbler',\n",
       " 'stupid',\n",
       " 'ma5terb8',\n",
       " 'suck',\n",
       " 'bigots',\n",
       " 'faggotcock',\n",
       " 'strippers',\n",
       " 'torment',\n",
       " 's_h_i_t',\n",
       " 'untruthful',\n",
       " 'dickface',\n",
       " 'tits',\n",
       " 'worthless',\n",
       " 'beastiality',\n",
       " 'negligent',\n",
       " 'dweeb',\n",
       " 'inept',\n",
       " 'cum',\n",
       " 'witless',\n",
       " 'ridicule',\n",
       " 'f',\n",
       " 'pisses',\n",
       " 'aggressive',\n",
       " 'unredemptive',\n",
       " 'teez',\n",
       " 'hooker',\n",
       " 'rimjob',\n",
       " 'dirty',\n",
       " 'inane',\n",
       " 'f**k',\n",
       " 'fuckwhit',\n",
       " 'loon',\n",
       " 'minge',\n",
       " 'baloney',\n",
       " 'chink',\n",
       " 'craw',\n",
       " 'perjurs',\n",
       " 'braindead',\n",
       " 'molested',\n",
       " 'dummazz',\n",
       " 'reproduce',\n",
       " 'zealot',\n",
       " 'silliness',\n",
       " 'fuckin',\n",
       " 'assfukka',\n",
       " 'fornicating',\n",
       " 'fukwit',\n",
       " 'fools',\n",
       " 'invaders',\n",
       " 'rediculous',\n",
       " 'bestiality',\n",
       " 'fagfucker',\n",
       " 'vomit',\n",
       " 'smelly',\n",
       " 'orgy',\n",
       " 'basterdization',\n",
       " 'niglet',\n",
       " 'mouthbreathing',\n",
       " 'egotist',\n",
       " 'fuks',\n",
       " 'low-iq',\n",
       " 'sleaze',\n",
       " 'fking',\n",
       " 'dumbed',\n",
       " 'closeminded',\n",
       " 'snake',\n",
       " 'penisextension',\n",
       " 'scumbag',\n",
       " 'racism',\n",
       " 'shitter',\n",
       " 'naked',\n",
       " 'assbag',\n",
       " 'sick',\n",
       " 'petullent',\n",
       " 'chickenshit',\n",
       " 'puto',\n",
       " 'gringo',\n",
       " 'unappealing',\n",
       " 'sillie',\n",
       " 'punch',\n",
       " 'ignorance',\n",
       " 'assnigger',\n",
       " 'bitchers',\n",
       " 'cow',\n",
       " 'faggs',\n",
       " 'uck',\n",
       " 'mcfagget',\n",
       " 'scrub',\n",
       " 'fucks',\n",
       " 'parasitic',\n",
       " 'conniving',\n",
       " 'disgrace',\n",
       " 'immorality',\n",
       " 'ejaculation',\n",
       " 'baffoon',\n",
       " 'damning',\n",
       " 'shitings',\n",
       " 'delusive',\n",
       " 'xenophobe',\n",
       " 'a-holes',\n",
       " 'dishonest',\n",
       " 'sexist',\n",
       " 'thugs',\n",
       " \"idiots'\",\n",
       " 'jerk-off',\n",
       " 'hate-filled',\n",
       " 'anti-lgbt',\n",
       " 'corrupt',\n",
       " 'knob',\n",
       " 'smut',\n",
       " 'rathole',\n",
       " 'reek',\n",
       " 'h-o-l-e',\n",
       " 'fellatio',\n",
       " 'darn',\n",
       " 'asswhole',\n",
       " 'jizm',\n",
       " 'fat',\n",
       " 'sycophantic',\n",
       " 'stupididty',\n",
       " 'puke',\n",
       " 'horny',\n",
       " 'knuckleheads',\n",
       " 'maladroit',\n",
       " 'jack-off',\n",
       " 'jackass',\n",
       " 'wankjob',\n",
       " 'numbskull',\n",
       " 'attacker',\n",
       " 'eradicated',\n",
       " 'dickmonger',\n",
       " 'shitcunt',\n",
       " 'sanctimonious',\n",
       " 'semen',\n",
       " 'fagging',\n",
       " 'perverted',\n",
       " 'libtards',\n",
       " 'ludicrously',\n",
       " 'raving',\n",
       " 'bi+ch',\n",
       " 'foxy',\n",
       " 'betrayed',\n",
       " 'sickly',\n",
       " 'lying',\n",
       " 'shitting',\n",
       " 'poon',\n",
       " 'sucky',\n",
       " 'buttercup',\n",
       " 'deggo',\n",
       " 'scum',\n",
       " 'motherfuckings',\n",
       " 'antimilitary',\n",
       " 'dummy',\n",
       " 'infantile',\n",
       " 'hotsex',\n",
       " 'penisbanger',\n",
       " 'massacre',\n",
       " 'fukin',\n",
       " 'kondum',\n",
       " 'mutha',\n",
       " 'nipples',\n",
       " 'knobead',\n",
       " '4r5e',\n",
       " 'genocide',\n",
       " 'lesbo',\n",
       " 'sacks',\n",
       " 'shitdick',\n",
       " 'lesbians',\n",
       " 'cowards',\n",
       " 'schumucks',\n",
       " 'junk',\n",
       " 'bitchy',\n",
       " 'traitorous',\n",
       " 'mental',\n",
       " 'titwank',\n",
       " 's hit',\n",
       " 'gangbang',\n",
       " 'cunt',\n",
       " 'sucking',\n",
       " 'headless',\n",
       " 'yates',\n",
       " 'threats',\n",
       " 'detest',\n",
       " 'queerhole',\n",
       " 'gook',\n",
       " 'bonkers',\n",
       " 'spouting',\n",
       " 'grifters',\n",
       " 'shpremes',\n",
       " 'c0cksucker',\n",
       " 'hypocritical',\n",
       " 'anti-catholic',\n",
       " 'vapidity',\n",
       " 'buffoon',\n",
       " 'ballsack',\n",
       " 'cyberfuck',\n",
       " 'dimwit',\n",
       " 'filthy',\n",
       " 'nuts',\n",
       " 'smucks',\n",
       " 'cocknugget',\n",
       " 'cheat',\n",
       " 'assjacker',\n",
       " 'shit',\n",
       " 'extremist',\n",
       " 'beaner',\n",
       " 'corrup',\n",
       " 'bammers',\n",
       " 'naziand',\n",
       " 'silly',\n",
       " 'miserably',\n",
       " 'knobhead',\n",
       " 'kike',\n",
       " 'senile',\n",
       " 'trickster',\n",
       " 'donutholes',\n",
       " 'dick',\n",
       " 'bellicose',\n",
       " 'fuckshitcunt',\n",
       " 'narrow-minded',\n",
       " 'horseshit',\n",
       " 'psycho-killers',\n",
       " 'motherfucker',\n",
       " 'dyke',\n",
       " 'mitch',\n",
       " 'lon',\n",
       " 'disgusting',\n",
       " 'spewed',\n",
       " 'communists',\n",
       " 'slutbag',\n",
       " 'pegged',\n",
       " 'twunter',\n",
       " 'punta',\n",
       " 'cesspool',\n",
       " 'clit',\n",
       " 'horniest',\n",
       " 'shits',\n",
       " 'n1gger',\n",
       " 'knobjocky',\n",
       " 'stupidis',\n",
       " 'nutter',\n",
       " 'schill',\n",
       " 'primitive',\n",
       " 'rancid',\n",
       " 'pornos',\n",
       " 'cretinous',\n",
       " 'aholes',\n",
       " 'transvestite',\n",
       " 'twit',\n",
       " 'assmonkey',\n",
       " 'grifter',\n",
       " 'horrific',\n",
       " 'stupidism',\n",
       " 'bootlicking',\n",
       " 'trumpster',\n",
       " 'blood-sucking',\n",
       " 'un-smart',\n",
       " 'trumpsters',\n",
       " 'meterosexual',\n",
       " 'shitey',\n",
       " 'losers',\n",
       " 'shitass',\n",
       " 'rooskies',\n",
       " 'axwound',\n",
       " 'pig',\n",
       " 'cuntslut',\n",
       " 'pron',\n",
       " 'sycophant',\n",
       " \"mothafuckin\\\\'\",\n",
       " 'mothafuck',\n",
       " 'trumplers',\n",
       " 'ahole',\n",
       " 'deceptive',\n",
       " 'peanuthead',\n",
       " 'rear',\n",
       " 'maniacal',\n",
       " 'sh**',\n",
       " 'insane',\n",
       " 'wank',\n",
       " 'phuking',\n",
       " 'demented',\n",
       " 'bastardos',\n",
       " 'fuckbrain',\n",
       " 'voetsek',\n",
       " 'burning',\n",
       " 'sht',\n",
       " 'nobjocky',\n",
       " 'fuckwit',\n",
       " 'clits',\n",
       " 'man-boy',\n",
       " 'deviant',\n",
       " 'putrid',\n",
       " 'stoned',\n",
       " 'brotherfucker',\n",
       " 'pusssy',\n",
       " 'disgusti',\n",
       " 'knobjokey',\n",
       " 'eco-radicals',\n",
       " 'sucker',\n",
       " 'damned',\n",
       " 'fuckface',\n",
       " 'meanspirited',\n",
       " 'dirsa',\n",
       " 'moochers',\n",
       " 'terrorist',\n",
       " 'loudmouth',\n",
       " 'islamaphobeliar',\n",
       " 'kicking',\n",
       " 'cretins',\n",
       " 'porn',\n",
       " 'wimpy',\n",
       " 'ugly',\n",
       " 'nutjobs',\n",
       " 'peo',\n",
       " 'fabricator',\n",
       " 'vermin',\n",
       " 'shoot',\n",
       " 'hoare',\n",
       " 'dlck',\n",
       " 'tittywank',\n",
       " 'perverts',\n",
       " 'weird',\n",
       " 'assclown',\n",
       " 'phukking',\n",
       " 'nards',\n",
       " 'dumb ass',\n",
       " 'dickweed',\n",
       " 'cumming',\n",
       " 'dickfucker',\n",
       " 'stupidest',\n",
       " 'queerbait',\n",
       " 'crap',\n",
       " 'fingerfucking',\n",
       " 'lousy',\n",
       " 'slave',\n",
       " 'fink',\n",
       " 'cums',\n",
       " 'treacherous',\n",
       " 'exterminate',\n",
       " 'dumbeddown',\n",
       " 'f***ing',\n",
       " 'shaggin',\n",
       " 'tard',\n",
       " 'pornography',\n",
       " 'tw4t',\n",
       " 'honkey',\n",
       " 'vandalize',\n",
       " 'sucked',\n",
       " 'transgender',\n",
       " 'nigg4h',\n",
       " 'islamophobia',\n",
       " 'f4nny',\n",
       " 'mudered',\n",
       " 'pedo',\n",
       " 'nut sack',\n",
       " 'dck',\n",
       " 'threatened',\n",
       " 'idiota',\n",
       " 'm0f0',\n",
       " 'wimp',\n",
       " 'infiltrator',\n",
       " 's**t',\n",
       " 'invading',\n",
       " 'mean-spirited',\n",
       " 'homosexuals',\n",
       " 'redneck',\n",
       " 'heinous',\n",
       " 'butt',\n",
       " 'bulls**tter',\n",
       " 'vicious',\n",
       " 'asssucker',\n",
       " 'beeotc',\n",
       " 'slugs',\n",
       " 'lame',\n",
       " 'stonestupid',\n",
       " 'shitbagger',\n",
       " 'nazis',\n",
       " 'racists',\n",
       " 'stupib',\n",
       " 'antiisrael',\n",
       " 'shitface',\n",
       " 'acismstupidity',\n",
       " 'hypocrats',\n",
       " 'gluttons',\n",
       " 'crock',\n",
       " 'lickers',\n",
       " 'fucktart',\n",
       " 'morons',\n",
       " 'pee',\n",
       " 'crooks',\n",
       " 'insolent',\n",
       " 'goof',\n",
       " 'isl',\n",
       " 'buceta',\n",
       " 'clitoris',\n",
       " 'dumba',\n",
       " 'dickbeaters',\n",
       " 'jiz',\n",
       " 'clapper',\n",
       " 'fooled',\n",
       " 'gaza',\n",
       " 'goatse',\n",
       " 'pedophile',\n",
       " 'pussy',\n",
       " 'viciously',\n",
       " 'cumshot',\n",
       " 'dweebs',\n",
       " 'cuck',\n",
       " 'ill',\n",
       " 'mothafuckas',\n",
       " \"f'king\",\n",
       " 'kootch',\n",
       " 'whorebag',\n",
       " 'lucifer',\n",
       " 'shitted',\n",
       " 'undependable',\n",
       " 'crooked',\n",
       " 'heeb',\n",
       " 'loyalists',\n",
       " 'porch monkey',\n",
       " 'willy',\n",
       " 'lameass',\n",
       " 'pseudo-human',\n",
       " 'dumbie',\n",
       " 'beeotch',\n",
       " 'dbag',\n",
       " \"fool's\",\n",
       " \"satan's\",\n",
       " 'coke',\n",
       " 'pedophiles',\n",
       " 'penetrating',\n",
       " 'cocksniffer',\n",
       " 'twatty',\n",
       " 'brat',\n",
       " 'hed',\n",
       " 'dog-fucker',\n",
       " 'fanyy',\n",
       " 'passiveaggresive',\n",
       " 'p****',\n",
       " 'assfuck',\n",
       " 'dotard',\n",
       " 'numbnuts',\n",
       " 'paranoid',\n",
       " 'poonani',\n",
       " 'devil',\n",
       " 'a*****es',\n",
       " 'stupidfat',\n",
       " 'fistfucks',\n",
       " 'testicle',\n",
       " 'freaking',\n",
       " 'fuckers',\n",
       " 'self-fornication',\n",
       " 'ignoramous',\n",
       " 'Goddamn',\n",
       " 'penis-extension',\n",
       " 'scum-sucking',\n",
       " 'mick',\n",
       " 'whiteys',\n",
       " 'muslim',\n",
       " 'vultures',\n",
       " 'pigs',\n",
       " 'selfish',\n",
       " 'blow',\n",
       " 'moronic',\n",
       " 'blow job',\n",
       " 'gaybob',\n",
       " 'stalker',\n",
       " 'bat***',\n",
       " 'ass-pirate',\n",
       " 'junglebunny',\n",
       " 'boobs',\n",
       " 'spunk',\n",
       " 'raped',\n",
       " 'rats',\n",
       " 'cocksukka',\n",
       " 'cockmongruel',\n",
       " 'gaywad',\n",
       " 'cox',\n",
       " 'a****le',\n",
       " 'dicks',\n",
       " 'trump-suckers',\n",
       " 'boorish',\n",
       " 'nasty',\n",
       " 'trappers',\n",
       " 'lowlife',\n",
       " 'fcuker',\n",
       " \"rat's\",\n",
       " 'apd',\n",
       " 'supremacist',\n",
       " 'whitefragilitycankissmyass',\n",
       " 'scummy',\n",
       " 'phukked',\n",
       " 'poop',\n",
       " 'squirt',\n",
       " 'trumpkin',\n",
       " 'shagger',\n",
       " 'idiots',\n",
       " 'meanspiritedness',\n",
       " 'white-trash',\n",
       " 'cumtart',\n",
       " 'degenerates',\n",
       " 'dickfuck',\n",
       " 'cockface',\n",
       " 'idiocracy',\n",
       " 'deviants',\n",
       " 'jingoistic',\n",
       " 'fuckhead',\n",
       " 'immoral',\n",
       " 'deceitful',\n",
       " 'queef',\n",
       " 'zombies',\n",
       " 'oafish',\n",
       " 'dimwitted',\n",
       " 'anus',\n",
       " 'scammers',\n",
       " 'skullfuck',\n",
       " 'filth',\n",
       " 'lardass',\n",
       " 'douchebag',\n",
       " 'tosser',\n",
       " 'pawn',\n",
       " 'douchebags',\n",
       " 'anti-religious',\n",
       " 'wwiii',\n",
       " 'burns',\n",
       " 'sleazy',\n",
       " 's.o.b.',\n",
       " 'orgasims',\n",
       " 'chesticle',\n",
       " 'madmen',\n",
       " 'cyberfuckers',\n",
       " 'junky',\n",
       " 'niggaz',\n",
       " 'leftist',\n",
       " 'clitfuck',\n",
       " 'unruly',\n",
       " 'canucks',\n",
       " 'humbug',\n",
       " 'purp',\n",
       " 'niggas',\n",
       " 'mentally',\n",
       " 'sand nigger',\n",
       " 'tran',\n",
       " 'stinks',\n",
       " 'a**holes',\n",
       " 'faggot',\n",
       " 'pu*ssy',\n",
       " 'bampot',\n",
       " 'babbling',\n",
       " 'bitching',\n",
       " 'selffornication',\n",
       " 'unrational',\n",
       " 'rabidly',\n",
       " 'ass-fucker',\n",
       " 'opportunistic',\n",
       " 'pricks',\n",
       " 'icky',\n",
       " 'clown',\n",
       " 'fingerfucker',\n",
       " 'fooker',\n",
       " 'gangbangs',\n",
       " 'shitheads',\n",
       " 'murdered',\n",
       " 'dunce',\n",
       " 'assfucker',\n",
       " 'goofs',\n",
       " 'kraut',\n",
       " 'std',\n",
       " 'scrotum',\n",
       " 'goofiest',\n",
       " 'hoe',\n",
       " 'mothafuckin',\n",
       " 'kummer',\n",
       " 'hating',\n",
       " \"bstrd's\",\n",
       " 'masterbate',\n",
       " 'masturbate',\n",
       " 'asses',\n",
       " 'narcissists',\n",
       " 'poo',\n",
       " 'kunt',\n",
       " 'goddamned',\n",
       " 'bastard',\n",
       " 'knucklehead',\n",
       " 'dumpster',\n",
       " 'bollock',\n",
       " 'cocksmoke',\n",
       " 'niggardly',\n",
       " 'unwashed',\n",
       " 'useles',\n",
       " 'bum',\n",
       " 'cockmongler',\n",
       " 'd-bag',\n",
       " 'boot-licking',\n",
       " 'brays',\n",
       " 'ludicrous',\n",
       " 'cock-sucker',\n",
       " 'mysogynistic',\n",
       " 'furhrer',\n",
       " 'pussies',\n",
       " 'rubbish',\n",
       " 'acist',\n",
       " 'pissed',\n",
       " 'f*c*',\n",
       " 'killing',\n",
       " 'damnable',\n",
       " 'vagrancy',\n",
       " 'crook',\n",
       " 'lmfao',\n",
       " 'homodumbshit',\n",
       " 'scumsucking',\n",
       " 't1tties',\n",
       " 'meth',\n",
       " 'skinheads',\n",
       " 'crotch',\n",
       " 'shiznit',\n",
       " 'xenophobes',\n",
       " 'degenerate',\n",
       " 'fetid',\n",
       " 'fricking',\n",
       " 'kum',\n",
       " \"idiot's\",\n",
       " 'beating',\n",
       " 'antichrist',\n",
       " 'phony',\n",
       " 'fuckheads',\n",
       " 'xenophobic',\n",
       " 'asscock',\n",
       " 'friggen',\n",
       " 'cyberfuc',\n",
       " 'trump-humpers',\n",
       " '5hit',\n",
       " 'sluts',\n",
       " 'simpleton',\n",
       " 'crystal',\n",
       " 'feminazis',\n",
       " 'racist',\n",
       " 'hoar',\n",
       " 'trashtalk',\n",
       " 'hypocritica',\n",
       " 'vagina',\n",
       " 'looser',\n",
       " 'slug',\n",
       " 'ruining',\n",
       " 'breasts',\n",
       " 'goldigger',\n",
       " 'jerk off',\n",
       " 'bstard',\n",
       " 'phuq',\n",
       " 'fags',\n",
       " 'gay-haters',\n",
       " 'baztarbs',\n",
       " 'bitchin',\n",
       " 'jigaboo',\n",
       " 'jungle bunny',\n",
       " 'butthole',\n",
       " 'lowiq',\n",
       " 'cunilingus',\n",
       " 'phuks',\n",
       " 'stupidly',\n",
       " 'arrogant',\n",
       " 'pecker',\n",
       " 'wtf',\n",
       " 'criminals',\n",
       " 'hermit',\n",
       " 'b1tch',\n",
       " 'psychotic',\n",
       " 'bastards',\n",
       " 'tittyfuck',\n",
       " 'fatties',\n",
       " 'motherfuckin',\n",
       " 'repugnant',\n",
       " 'terrible',\n",
       " 'buttplug',\n",
       " 'p*ssies',\n",
       " 'foolish',\n",
       " 'lackeys',\n",
       " 'fuckass',\n",
       " 'loony',\n",
       " 'butt-pirate',\n",
       " 'pedophilia',\n",
       " 'knobend',\n",
       " 'gaylord',\n",
       " 'dumbweazy',\n",
       " 'thug',\n",
       " 'porno',\n",
       " 'n1gga',\n",
       " 'thoughtless',\n",
       " 'clownbeast',\n",
       " 'penisfucker',\n",
       " 'redenbacher',\n",
       " 'kidnap',\n",
       " 'shameless',\n",
       " 'incompetents',\n",
       " 'cockknoker',\n",
       " 'ignoramus',\n",
       " 'asswipe',\n",
       " 'cocksuck',\n",
       " 'clueless',\n",
       " 'fakenews',\n",
       " 'sociopath',\n",
       " 'scrote',\n",
       " 'cnut',\n",
       " 'reich',\n",
       " 'oppressive',\n",
       " 'queer',\n",
       " 'cumbubble',\n",
       " 'embarrassment',\n",
       " 'chump',\n",
       " 'mindless',\n",
       " 'nutsack',\n",
       " 'cocksmoker',\n",
       " 'dickwad',\n",
       " 'retard',\n",
       " 'masochist',\n",
       " 'goddamnit',\n",
       " 'cockmaster',\n",
       " 'classless',\n",
       " 'liar',\n",
       " 'clowns',\n",
       " 'fux0r',\n",
       " 'thief',\n",
       " 'murders',\n",
       " 'coochy',\n",
       " 'imbecile',\n",
       " 'close-minded',\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"dictionaries/toxic_words.txt\") as toxic_words_file:\n",
    "    set_of_toxic_words: set = set([word.strip() for word in toxic_words_file.readlines()])\n",
    "set_of_toxic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9291c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def dictionary_analysis_over_set_intersection(dict_name: str, dict_set: set, data: pd.DataFrame) -> Tuple[pd.DataFrame, Counter]:\n",
    "    dict_word_counter: Counter = Counter()\n",
    "    dict_word_count: list = []\n",
    "    for row in data.text:\n",
    "        dict_words_in_comment: set = set(word_tokenize(row)).intersection(dict_set)\n",
    "        dict_word_counter.update(dict_words_in_comment)\n",
    "        dict_word_count.append(len(dict_words_in_comment))\n",
    "    data[f\"{dict_name}_word_count\"] = dict_word_count\n",
    "    return data, dict_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed16de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>toxic_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgwRx2dFc5pnjsGjCdh4AaABAg.9VVdHdnh2yK9VYnTuEtgR-</td>\n",
       "      <td>It&amp;#39;s become such a meme that everyone star...</td>\n",
       "      <td>UC6X1kUf-avVsUgveP9JJnlQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-05T01:26:20Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgzKS4j0zSepgYatXqF4AaABAg.9VVc8vc96Mb9VXxGtDQcPc</td>\n",
       "      <td>@Nikolay Panayotov no i would not care . as lo...</td>\n",
       "      <td>UC9NuVWBlxdueOgoXJOZQFkQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-04T17:32:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgyHMakWj8e1MZS4Bi94AaABAg.9VVdgPwr1Pk9V_MNHbB9Zx</td>\n",
       "      <td>Without him, the front of the field would be l...</td>\n",
       "      <td>UCJeq6aCoT2Os5F7-Li2GYfw</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-05T15:59:15Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgwDLBFFYyM-F_WxQkV4AaABAg.9VVcaRNVn0H9VVqx-nSSmz</td>\n",
       "      <td>@PolarPenguin How was he robbed of points in B...</td>\n",
       "      <td>UCJOoF1lDfv39IpAHMfz8nQA</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-12-03T21:58:56Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgwDLBFFYyM-F_WxQkV4AaABAg.9VVcaRNVn0H9VVp3onuetR</td>\n",
       "      <td>@PolarPenguin How can you be robbed of points ...</td>\n",
       "      <td>UCp9qNkL-YwL8dPrKi2eiQNQ</td>\n",
       "      <td>16</td>\n",
       "      <td>2021-12-03T21:42:32Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40128</th>\n",
       "      <td>v6asa9zegAs</td>\n",
       "      <td>Ugzvsp_8rQ224V7hfQ54AaABAg.9YaMQ52NxsE9YhkCCUHtbI</td>\n",
       "      <td>The side looks like a fat guy sat on it but be...</td>\n",
       "      <td>UCtauTDQYf6h_8c-9ty8rCJA</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-21T11:44:18Z</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40135</th>\n",
       "      <td>v6asa9zegAs</td>\n",
       "      <td>UgysXm_H7fFhGvm9OlN4AaABAg.9Y_pOrDe4HI9Ya3t0F6Uq6</td>\n",
       "      <td>@Kevin Mazariegos I was thinking the same. It ...</td>\n",
       "      <td>UCnn1N9y6zV2Kvx91za3E0rw</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-18T12:11:07Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40144</th>\n",
       "      <td>v6asa9zegAs</td>\n",
       "      <td>UgwtsMLjjhALR8KB-9R4AaABAg.9Y_qrj8kYrc9Y_v4wOinCF</td>\n",
       "      <td>The car is gonna insane</td>\n",
       "      <td>UCgZUEWZBPy-X2fkj1Edu0hg</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-18T10:45:30Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40162</th>\n",
       "      <td>v6asa9zegAs</td>\n",
       "      <td>UgzLGWxK7PbHQ0TMfKB4AaABAg</td>\n",
       "      <td>George touching the rear wing when they walk u...</td>\n",
       "      <td>UCf3jvMJiCAQy7OLJTQjgq1Q</td>\n",
       "      <td>204</td>\n",
       "      <td>2022-02-18T10:17:20Z</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40169</th>\n",
       "      <td>v6asa9zegAs</td>\n",
       "      <td>UgzLGWxK7PbHQ0TMfKB4AaABAg.9Y_rrgbpmjx9Y_w9ABH1rz</td>\n",
       "      <td>@Tomisin Esan- George Last season Max touched ...</td>\n",
       "      <td>UCiVLYJPtONXcd0l6dLv1rTQ</td>\n",
       "      <td>32</td>\n",
       "      <td>2022-02-18T10:54:49Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2497 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                                 id  \\\n",
       "16     FZjG3oft5rs  UgwRx2dFc5pnjsGjCdh4AaABAg.9VVdHdnh2yK9VYnTuEtgR-   \n",
       "26     FZjG3oft5rs  UgzKS4j0zSepgYatXqF4AaABAg.9VVc8vc96Mb9VXxGtDQcPc   \n",
       "36     FZjG3oft5rs  UgyHMakWj8e1MZS4Bi94AaABAg.9VVdgPwr1Pk9V_MNHbB9Zx   \n",
       "65     FZjG3oft5rs  UgwDLBFFYyM-F_WxQkV4AaABAg.9VVcaRNVn0H9VVqx-nSSmz   \n",
       "66     FZjG3oft5rs  UgwDLBFFYyM-F_WxQkV4AaABAg.9VVcaRNVn0H9VVp3onuetR   \n",
       "...            ...                                                ...   \n",
       "40128  v6asa9zegAs  Ugzvsp_8rQ224V7hfQ54AaABAg.9YaMQ52NxsE9YhkCCUHtbI   \n",
       "40135  v6asa9zegAs  UgysXm_H7fFhGvm9OlN4AaABAg.9Y_pOrDe4HI9Ya3t0F6Uq6   \n",
       "40144  v6asa9zegAs  UgwtsMLjjhALR8KB-9R4AaABAg.9Y_qrj8kYrc9Y_v4wOinCF   \n",
       "40162  v6asa9zegAs                         UgzLGWxK7PbHQ0TMfKB4AaABAg   \n",
       "40169  v6asa9zegAs  UgzLGWxK7PbHQ0TMfKB4AaABAg.9Y_rrgbpmjx9Y_w9ABH1rz   \n",
       "\n",
       "                                                    text  \\\n",
       "16     It&#39;s become such a meme that everyone star...   \n",
       "26     @Nikolay Panayotov no i would not care . as lo...   \n",
       "36     Without him, the front of the field would be l...   \n",
       "65     @PolarPenguin How was he robbed of points in B...   \n",
       "66     @PolarPenguin How can you be robbed of points ...   \n",
       "...                                                  ...   \n",
       "40128  The side looks like a fat guy sat on it but be...   \n",
       "40135  @Kevin Mazariegos I was thinking the same. It ...   \n",
       "40144                            The car is gonna insane   \n",
       "40162  George touching the rear wing when they walk u...   \n",
       "40169  @Tomisin Esan- George Last season Max touched ...   \n",
       "\n",
       "                           user  like_count          published_at  \\\n",
       "16     UC6X1kUf-avVsUgveP9JJnlQ           1  2021-12-05T01:26:20Z   \n",
       "26     UC9NuVWBlxdueOgoXJOZQFkQ           1  2021-12-04T17:32:42Z   \n",
       "36     UCJeq6aCoT2Os5F7-Li2GYfw           1  2021-12-05T15:59:15Z   \n",
       "65     UCJOoF1lDfv39IpAHMfz8nQA           2  2021-12-03T21:58:56Z   \n",
       "66     UCp9qNkL-YwL8dPrKi2eiQNQ          16  2021-12-03T21:42:32Z   \n",
       "...                         ...         ...                   ...   \n",
       "40128  UCtauTDQYf6h_8c-9ty8rCJA           0  2022-02-21T11:44:18Z   \n",
       "40135  UCnn1N9y6zV2Kvx91za3E0rw           0  2022-02-18T12:11:07Z   \n",
       "40144  UCgZUEWZBPy-X2fkj1Edu0hg           1  2022-02-18T10:45:30Z   \n",
       "40162  UCf3jvMJiCAQy7OLJTQjgq1Q         204  2022-02-18T10:17:20Z   \n",
       "40169  UCiVLYJPtONXcd0l6dLv1rTQ          32  2022-02-18T10:54:49Z   \n",
       "\n",
       "       reply_count  toxic_word_count  \n",
       "16               0                 1  \n",
       "26               0                 2  \n",
       "36               0                 1  \n",
       "65               0                 1  \n",
       "66               0                 1  \n",
       "...            ...               ...  \n",
       "40128            0                 2  \n",
       "40135            0                 1  \n",
       "40144            0                 1  \n",
       "40162            9                 1  \n",
       "40169            0                 1  \n",
       "\n",
       "[2497 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df, toxic_word_counter = dictionary_analysis_over_set_intersection(dict_name=\"toxic\", dict_set=set_of_toxic_words, data=comment_df)\n",
    "comment_df.loc[comment_df[\"toxic_word_count\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bc274f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'dumb': 67,\n",
       "         'murdering': 2,\n",
       "         'f': 26,\n",
       "         'threats': 3,\n",
       "         'robbed': 19,\n",
       "         'highs': 12,\n",
       "         'fanboy': 57,\n",
       "         'brat': 10,\n",
       "         'arrogant': 18,\n",
       "         'cheating': 57,\n",
       "         'ridiculous': 34,\n",
       "         'crazy': 133,\n",
       "         'beaten': 51,\n",
       "         'steal': 4,\n",
       "         'useless': 23,\n",
       "         'weird': 79,\n",
       "         'God': 7,\n",
       "         'cheat': 27,\n",
       "         'dumpster': 1,\n",
       "         'mafia': 4,\n",
       "         'ignorant': 15,\n",
       "         'lying': 17,\n",
       "         'rear': 79,\n",
       "         'idiot': 42,\n",
       "         'dumbest': 10,\n",
       "         'hell': 8,\n",
       "         'ass': 5,\n",
       "         'stupid': 11,\n",
       "         'puppet': 3,\n",
       "         'morons': 4,\n",
       "         'clown': 40,\n",
       "         'damn': 4,\n",
       "         'idiotic': 17,\n",
       "         'terrible': 76,\n",
       "         'fuck': 4,\n",
       "         'pissed': 2,\n",
       "         'garbage': 23,\n",
       "         'fucking': 5,\n",
       "         'messed': 32,\n",
       "         'fat': 6,\n",
       "         'weak': 22,\n",
       "         'sucker': 2,\n",
       "         'insane': 115,\n",
       "         'duh': 8,\n",
       "         'unpopular': 3,\n",
       "         'kills': 7,\n",
       "         'fake': 38,\n",
       "         'cursed': 7,\n",
       "         'sick': 22,\n",
       "         'badass': 5,\n",
       "         'fails': 25,\n",
       "         'con': 27,\n",
       "         'freaking': 9,\n",
       "         'pig': 2,\n",
       "         'fool': 24,\n",
       "         'racism': 18,\n",
       "         'chicken': 8,\n",
       "         'foolish': 4,\n",
       "         'murdered': 4,\n",
       "         'hypocritical': 4,\n",
       "         'cow': 4,\n",
       "         'hating': 37,\n",
       "         'kicked': 18,\n",
       "         'trash': 39,\n",
       "         'silly': 36,\n",
       "         'inferior': 25,\n",
       "         'l': 26,\n",
       "         'miserably': 1,\n",
       "         'fools': 6,\n",
       "         'racists': 3,\n",
       "         'ignorance': 7,\n",
       "         'selfish': 24,\n",
       "         'fk': 6,\n",
       "         'fkn': 4,\n",
       "         'disgusting': 13,\n",
       "         'racist': 23,\n",
       "         'laughable': 7,\n",
       "         'nasty': 13,\n",
       "         'looser': 4,\n",
       "         'aggressive': 59,\n",
       "         'ruining': 22,\n",
       "         'beating': 42,\n",
       "         'shootout': 4,\n",
       "         'mentally': 14,\n",
       "         'conman': 1,\n",
       "         'dead': 8,\n",
       "         'deluded': 13,\n",
       "         'threatened': 7,\n",
       "         'bullshit': 1,\n",
       "         'stfu': 1,\n",
       "         'idiots': 15,\n",
       "         'butt': 1,\n",
       "         'lmao': 18,\n",
       "         'clueless': 15,\n",
       "         'diehard': 4,\n",
       "         'irrational': 3,\n",
       "         'bums': 3,\n",
       "         'hypocrite': 5,\n",
       "         'choke': 8,\n",
       "         'cruel': 2,\n",
       "         'mediocre': 20,\n",
       "         'troll': 25,\n",
       "         'pathetic': 21,\n",
       "         'unstable': 6,\n",
       "         'immoral': 1,\n",
       "         'ridiculously': 4,\n",
       "         'killing': 25,\n",
       "         'shit': 9,\n",
       "         'lmfao': 1,\n",
       "         'suck': 3,\n",
       "         'brats': 1,\n",
       "         'kill': 1,\n",
       "         'rectum': 1,\n",
       "         'crap': 8,\n",
       "         'goddamn': 1,\n",
       "         'immature': 12,\n",
       "         'idiocy': 2,\n",
       "         'horrific': 1,\n",
       "         'homophobic': 4,\n",
       "         'mental': 22,\n",
       "         'lame': 11,\n",
       "         'junk': 2,\n",
       "         'lazy': 5,\n",
       "         'executed': 4,\n",
       "         'screw': 1,\n",
       "         'incompetence': 8,\n",
       "         'fucked': 2,\n",
       "         'retarded': 1,\n",
       "         'wtf': 1,\n",
       "         'donkey': 2,\n",
       "         'dishonest': 1,\n",
       "         'torture': 3,\n",
       "         'sucks': 1,\n",
       "         'bucko': 1,\n",
       "         'losers': 3,\n",
       "         'undeserving': 1,\n",
       "         'monster': 22,\n",
       "         'loser': 26,\n",
       "         'mad': 49,\n",
       "         'manipulating': 4,\n",
       "         'fricking': 4,\n",
       "         'vile': 4,\n",
       "         'impostor': 1,\n",
       "         'shills': 1,\n",
       "         'dictator': 1,\n",
       "         'crybaby': 14,\n",
       "         'snake': 2,\n",
       "         'nut': 3,\n",
       "         'sabotage': 4,\n",
       "         'shoot': 6,\n",
       "         'blow': 19,\n",
       "         'bozo': 2,\n",
       "         'trolls': 5,\n",
       "         'massacre': 1,\n",
       "         'psychopath': 2,\n",
       "         'terribly': 2,\n",
       "         'nuts': 9,\n",
       "         'dumber': 3,\n",
       "         'manchild': 5,\n",
       "         'fo': 1,\n",
       "         'dealers': 1,\n",
       "         'kick': 8,\n",
       "         'ridicule': 1,\n",
       "         'nobhead': 1,\n",
       "         'chick': 3,\n",
       "         'shits': 1,\n",
       "         'dictatorship': 3,\n",
       "         'treacherous': 4,\n",
       "         'punch': 5,\n",
       "         'bogus': 1,\n",
       "         'stupidity': 10,\n",
       "         'cult': 2,\n",
       "         'suffer': 12,\n",
       "         'rot': 1,\n",
       "         'mick': 3,\n",
       "         'scumbag': 1,\n",
       "         'coke': 4,\n",
       "         'senile': 1,\n",
       "         'whack': 2,\n",
       "         'ill': 8,\n",
       "         'whiny': 2,\n",
       "         'goon': 1,\n",
       "         'madness': 10,\n",
       "         'drunken': 1,\n",
       "         'invading': 1,\n",
       "         'punched': 2,\n",
       "         'incompetent': 1,\n",
       "         'uneducated': 1,\n",
       "         'braindead': 2,\n",
       "         'monsters': 2,\n",
       "         'suicide': 2,\n",
       "         'killer': 18,\n",
       "         'obnoxious': 2,\n",
       "         'pesky': 1,\n",
       "         'stink': 2,\n",
       "         'sociopath': 5,\n",
       "         'gag': 1,\n",
       "         'worthless': 8,\n",
       "         'fascist': 1,\n",
       "         'abused': 2,\n",
       "         'miserable': 6,\n",
       "         'horseshit': 2,\n",
       "         'stupidly': 3,\n",
       "         'xxx': 1,\n",
       "         'screwing': 1,\n",
       "         'ho': 1,\n",
       "         'clowns': 27,\n",
       "         'crappy': 2,\n",
       "         'chump': 2,\n",
       "         'petulant': 4,\n",
       "         'absurd': 5,\n",
       "         'trans': 1,\n",
       "         'opportunist': 1,\n",
       "         'disgrace': 4,\n",
       "         'fooled': 2,\n",
       "         'spouts': 2,\n",
       "         'misogynistic': 6,\n",
       "         'craziest': 7,\n",
       "         'snowflake': 2,\n",
       "         'bigotry': 3,\n",
       "         'gross': 3,\n",
       "         'witch': 2,\n",
       "         'poo': 1,\n",
       "         'pigs': 3,\n",
       "         'fukin': 1,\n",
       "         'tits': 1,\n",
       "         'piss': 1,\n",
       "         'sexist': 2,\n",
       "         'hopeless': 2,\n",
       "         'rat': 1,\n",
       "         'rabid': 1,\n",
       "         'terrorist': 2,\n",
       "         'muslim': 4,\n",
       "         'goofy': 7,\n",
       "         'scoundrel': 1,\n",
       "         'slug': 1,\n",
       "         'burning': 2,\n",
       "         'idiota': 1,\n",
       "         'bloody': 1,\n",
       "         'asinine': 3,\n",
       "         'disgust': 1,\n",
       "         'embarrassment': 2,\n",
       "         'hypocrisy': 5,\n",
       "         'damning': 1,\n",
       "         'hypocrites': 4,\n",
       "         'disease': 1,\n",
       "         'despicable': 4,\n",
       "         'nutjob': 1,\n",
       "         'oral': 1,\n",
       "         'clumsy': 1,\n",
       "         'dirty': 2,\n",
       "         'screwed': 2,\n",
       "         'punk': 1,\n",
       "         'dickhead': 1,\n",
       "         'dick': 1,\n",
       "         'dicks': 1,\n",
       "         'sucked': 1,\n",
       "         'inept': 2,\n",
       "         'vanity': 2,\n",
       "         'childlike': 1,\n",
       "         'ludicrous': 3,\n",
       "         'demented': 3,\n",
       "         'betrayed': 3,\n",
       "         'pretentious': 3,\n",
       "         'donkeys': 1,\n",
       "         'nutcase': 1,\n",
       "         'kicking': 4,\n",
       "         'misogynist': 1,\n",
       "         'crystal': 1,\n",
       "         'snowflakes': 2,\n",
       "         'potty': 1,\n",
       "         'foul': 2,\n",
       "         'sheeps': 2,\n",
       "         'kidnap': 2,\n",
       "         'stinky': 1,\n",
       "         'sickening': 1,\n",
       "         'smells': 1,\n",
       "         'burns': 4,\n",
       "         'coward': 1,\n",
       "         'lousy': 1,\n",
       "         'moronic': 1,\n",
       "         'headless': 2,\n",
       "         'sack': 3,\n",
       "         'crazies': 1,\n",
       "         'stupidest': 1,\n",
       "         'simpleton': 1,\n",
       "         'cuck': 1,\n",
       "         'dumbed': 1,\n",
       "         'demon': 3,\n",
       "         'dumba': 1,\n",
       "         'rediculous': 1,\n",
       "         'jan': 2,\n",
       "         'puppets': 2,\n",
       "         'cesspool': 2,\n",
       "         'whiner': 3,\n",
       "         'nincompoop': 2,\n",
       "         'slap': 2,\n",
       "         'reproduce': 2,\n",
       "         'bonkers': 2,\n",
       "         'corrupted': 1,\n",
       "         'misogynists': 1,\n",
       "         'criminals': 1,\n",
       "         'cringing': 1,\n",
       "         'scummy': 1,\n",
       "         'steals': 1,\n",
       "         'suckers': 1,\n",
       "         'ungrateful': 1,\n",
       "         'lone': 1,\n",
       "         'stinks': 1,\n",
       "         'evil': 1,\n",
       "         'crotch': 1,\n",
       "         'xenophobic': 3,\n",
       "         'narcissist': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_word_counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dac9531d",
   "metadata": {},
   "source": [
    "### Grievance Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9458b689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16bef80b9194c47b572a9c19a4dd3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/40200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>toxic_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg</td>\n",
       "      <td>that super max at the end wa fantast</td>\n",
       "      <td>UCmCBpDZeM9LbTeWbMPW1N2A</td>\n",
       "      <td>4626</td>\n",
       "      <td>2021-12-03T19:48:05Z</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD</td>\n",
       "      <td>@ iiillliii i got cold</td>\n",
       "      <td>UCQtwk7iBtv0hTwedp13yDKA</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-02T08:40:19Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9</td>\n",
       "      <td>@ miz is awesom oh ye.</td>\n",
       "      <td>UCuzQCuC86Pmhq9ixSff1z-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-01T18:23:44Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu</td>\n",
       "      <td>yea that is a fact, but if i would write like ...</td>\n",
       "      <td>UCgFRK81ZtgryEHR6GAxbWvQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-28T15:47:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW</td>\n",
       "      <td>@ miz is awesom didn ’ t age to well ehh?</td>\n",
       "      <td>UCADfAgrwLT4ZzpGkC8L-4gw</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-28T02:08:51Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40195</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgwZ6iauaq1cULpaUvR4AaABAg</td>\n",
       "      <td>how you manag juncao is so amaz . that ha to b...</td>\n",
       "      <td>UCmHLqIoiFsxOwsIEsqAVgzQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-16T11:01:58Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40196</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugxb7TesuwCHGhDxZRJ4AaABAg</td>\n",
       "      <td>absolut brillianc from seb on the radio as alw...</td>\n",
       "      <td>UCalYkEFZn7f91L316msDu7Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-25T16:01:06Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40197</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugx4RZlmzo8UHnkajsB4AaABAg</td>\n",
       "      <td>i &amp; #39; ll come back from time to time to wat...</td>\n",
       "      <td>UCYDZKuIlaaGAR-BOvs7W63Q</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T01:40:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40198</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgxbU_9pwAhxAIQkcRp4AaABAg</td>\n",
       "      <td>go to miss seb!</td>\n",
       "      <td>UCd4Bo6BxK0PHQnukaxeqnZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T21:19:20Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40199</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgyKB7N5-weM_DcpKbt4AaABAg</td>\n",
       "      <td>ferrari seb 2015-2018 nice but than how he fig...</td>\n",
       "      <td>UCylv5dmN-I2r7WxT2090rhg</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-19T20:23:04Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                                 id  \\\n",
       "0      FZjG3oft5rs                         UgznjAR9SoXoE0gpoK54AaABAg   \n",
       "1      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD   \n",
       "2      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9   \n",
       "3      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu   \n",
       "4      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW   \n",
       "...            ...                                                ...   \n",
       "40195  g5yQmp1ctXk                         UgwZ6iauaq1cULpaUvR4AaABAg   \n",
       "40196  g5yQmp1ctXk                         Ugxb7TesuwCHGhDxZRJ4AaABAg   \n",
       "40197  g5yQmp1ctXk                         Ugx4RZlmzo8UHnkajsB4AaABAg   \n",
       "40198  g5yQmp1ctXk                         UgxbU_9pwAhxAIQkcRp4AaABAg   \n",
       "40199  g5yQmp1ctXk                         UgyKB7N5-weM_DcpKbt4AaABAg   \n",
       "\n",
       "                                                    text  \\\n",
       "0                   that super max at the end wa fantast   \n",
       "1                                 @ iiillliii i got cold   \n",
       "2                                 @ miz is awesom oh ye.   \n",
       "3      yea that is a fact, but if i would write like ...   \n",
       "4              @ miz is awesom didn ’ t age to well ehh?   \n",
       "...                                                  ...   \n",
       "40195  how you manag juncao is so amaz . that ha to b...   \n",
       "40196  absolut brillianc from seb on the radio as alw...   \n",
       "40197  i & #39; ll come back from time to time to wat...   \n",
       "40198                                    go to miss seb!   \n",
       "40199  ferrari seb 2015-2018 nice but than how he fig...   \n",
       "\n",
       "                           user  like_count          published_at  \\\n",
       "0      UCmCBpDZeM9LbTeWbMPW1N2A        4626  2021-12-03T19:48:05Z   \n",
       "1      UCQtwk7iBtv0hTwedp13yDKA           0  2022-08-02T08:40:19Z   \n",
       "2      UCuzQCuC86Pmhq9ixSff1z-A           0  2022-02-01T18:23:44Z   \n",
       "3      UCgFRK81ZtgryEHR6GAxbWvQ           1  2021-12-28T15:47:42Z   \n",
       "4      UCADfAgrwLT4ZzpGkC8L-4gw           0  2021-12-28T02:08:51Z   \n",
       "...                         ...         ...                   ...   \n",
       "40195  UCmHLqIoiFsxOwsIEsqAVgzQ           0  2022-11-16T11:01:58Z   \n",
       "40196  UCalYkEFZn7f91L316msDu7Q           0  2022-11-25T16:01:06Z   \n",
       "40197  UCYDZKuIlaaGAR-BOvs7W63Q           1  2022-11-10T01:40:42Z   \n",
       "40198  UCd4Bo6BxK0PHQnukaxeqnZg           1  2022-11-10T21:19:20Z   \n",
       "40199  UCylv5dmN-I2r7WxT2090rhg           0  2022-12-19T20:23:04Z   \n",
       "\n",
       "       reply_count  toxic_word_count  \n",
       "0               48                 0  \n",
       "1                0                 0  \n",
       "2                0                 0  \n",
       "3                0                 0  \n",
       "4                0                 0  \n",
       "...            ...               ...  \n",
       "40195            0                 0  \n",
       "40196            0                 0  \n",
       "40197            0                 0  \n",
       "40198            0                 0  \n",
       "40199            0                 0  \n",
       "\n",
       "[40200 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk import TreebankWordDetokenizer\n",
    "import swifter\n",
    "\n",
    "stemmer: PorterStemmer = PorterStemmer()\n",
    "detokenizer: TreebankWordDetokenizer = TreebankWordDetokenizer()\n",
    "stemmed_comments: pd.DataFrame = comment_df.copy()\n",
    "stemmed_comments[\"text\"] = stemmed_comments.text.swifter.apply(lambda text: detokenizer.detokenize([stemmer.stem(word) for word in word_tokenize(text)]))\n",
    "stemmed_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141973b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deadline', 'desperation', 'fixation', 'frustration', 'god',\n",
       "       'grievance', 'hate', 'help', 'honour', 'impostor', 'jealousy',\n",
       "       'loneliness', 'murder', 'paranoia', 'planning', 'relationship',\n",
       "       'soldier', 'suicide', 'surveillance', 'threat', 'violence',\n",
       "       'weaponry'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "grievance_dict_df = pd.read_csv(\"dictionaries/grievancedictionary/dictionary_versions/dictionary_5plus.csv\")\n",
    "grievance_dict_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "categorys = grievance_dict_df.category.unique()\n",
    "categorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40dd748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>toxic_word_count</th>\n",
       "      <th>deadline_word_count</th>\n",
       "      <th>desperation_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>murder_word_count</th>\n",
       "      <th>paranoia_word_count</th>\n",
       "      <th>planning_word_count</th>\n",
       "      <th>relationship_word_count</th>\n",
       "      <th>soldier_word_count</th>\n",
       "      <th>suicide_word_count</th>\n",
       "      <th>surveillance_word_count</th>\n",
       "      <th>threat_word_count</th>\n",
       "      <th>violence_word_count</th>\n",
       "      <th>weaponry_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg</td>\n",
       "      <td>that super max at the end wa fantast</td>\n",
       "      <td>UCmCBpDZeM9LbTeWbMPW1N2A</td>\n",
       "      <td>4626</td>\n",
       "      <td>2021-12-03T19:48:05Z</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD</td>\n",
       "      <td>@ iiillliii i got cold</td>\n",
       "      <td>UCQtwk7iBtv0hTwedp13yDKA</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-02T08:40:19Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9</td>\n",
       "      <td>@ miz is awesom oh ye.</td>\n",
       "      <td>UCuzQCuC86Pmhq9ixSff1z-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-01T18:23:44Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu</td>\n",
       "      <td>yea that is a fact, but if i would write like ...</td>\n",
       "      <td>UCgFRK81ZtgryEHR6GAxbWvQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-28T15:47:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW</td>\n",
       "      <td>@ miz is awesom didn ’ t age to well ehh?</td>\n",
       "      <td>UCADfAgrwLT4ZzpGkC8L-4gw</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-28T02:08:51Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40195</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgwZ6iauaq1cULpaUvR4AaABAg</td>\n",
       "      <td>how you manag juncao is so amaz . that ha to b...</td>\n",
       "      <td>UCmHLqIoiFsxOwsIEsqAVgzQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-16T11:01:58Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40196</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugxb7TesuwCHGhDxZRJ4AaABAg</td>\n",
       "      <td>absolut brillianc from seb on the radio as alw...</td>\n",
       "      <td>UCalYkEFZn7f91L316msDu7Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-25T16:01:06Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40197</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugx4RZlmzo8UHnkajsB4AaABAg</td>\n",
       "      <td>i &amp; #39; ll come back from time to time to wat...</td>\n",
       "      <td>UCYDZKuIlaaGAR-BOvs7W63Q</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T01:40:42Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40198</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgxbU_9pwAhxAIQkcRp4AaABAg</td>\n",
       "      <td>go to miss seb!</td>\n",
       "      <td>UCd4Bo6BxK0PHQnukaxeqnZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T21:19:20Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40199</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgyKB7N5-weM_DcpKbt4AaABAg</td>\n",
       "      <td>ferrari seb 2015-2018 nice but than how he fig...</td>\n",
       "      <td>UCylv5dmN-I2r7WxT2090rhg</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-19T20:23:04Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40200 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                                 id  \\\n",
       "0      FZjG3oft5rs                         UgznjAR9SoXoE0gpoK54AaABAg   \n",
       "1      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD   \n",
       "2      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9   \n",
       "3      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu   \n",
       "4      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW   \n",
       "...            ...                                                ...   \n",
       "40195  g5yQmp1ctXk                         UgwZ6iauaq1cULpaUvR4AaABAg   \n",
       "40196  g5yQmp1ctXk                         Ugxb7TesuwCHGhDxZRJ4AaABAg   \n",
       "40197  g5yQmp1ctXk                         Ugx4RZlmzo8UHnkajsB4AaABAg   \n",
       "40198  g5yQmp1ctXk                         UgxbU_9pwAhxAIQkcRp4AaABAg   \n",
       "40199  g5yQmp1ctXk                         UgyKB7N5-weM_DcpKbt4AaABAg   \n",
       "\n",
       "                                                    text  \\\n",
       "0                   that super max at the end wa fantast   \n",
       "1                                 @ iiillliii i got cold   \n",
       "2                                 @ miz is awesom oh ye.   \n",
       "3      yea that is a fact, but if i would write like ...   \n",
       "4              @ miz is awesom didn ’ t age to well ehh?   \n",
       "...                                                  ...   \n",
       "40195  how you manag juncao is so amaz . that ha to b...   \n",
       "40196  absolut brillianc from seb on the radio as alw...   \n",
       "40197  i & #39; ll come back from time to time to wat...   \n",
       "40198                                    go to miss seb!   \n",
       "40199  ferrari seb 2015-2018 nice but than how he fig...   \n",
       "\n",
       "                           user  like_count          published_at  \\\n",
       "0      UCmCBpDZeM9LbTeWbMPW1N2A        4626  2021-12-03T19:48:05Z   \n",
       "1      UCQtwk7iBtv0hTwedp13yDKA           0  2022-08-02T08:40:19Z   \n",
       "2      UCuzQCuC86Pmhq9ixSff1z-A           0  2022-02-01T18:23:44Z   \n",
       "3      UCgFRK81ZtgryEHR6GAxbWvQ           1  2021-12-28T15:47:42Z   \n",
       "4      UCADfAgrwLT4ZzpGkC8L-4gw           0  2021-12-28T02:08:51Z   \n",
       "...                         ...         ...                   ...   \n",
       "40195  UCmHLqIoiFsxOwsIEsqAVgzQ           0  2022-11-16T11:01:58Z   \n",
       "40196  UCalYkEFZn7f91L316msDu7Q           0  2022-11-25T16:01:06Z   \n",
       "40197  UCYDZKuIlaaGAR-BOvs7W63Q           1  2022-11-10T01:40:42Z   \n",
       "40198  UCd4Bo6BxK0PHQnukaxeqnZg           1  2022-11-10T21:19:20Z   \n",
       "40199  UCylv5dmN-I2r7WxT2090rhg           0  2022-12-19T20:23:04Z   \n",
       "\n",
       "       reply_count  toxic_word_count  deadline_word_count  \\\n",
       "0               48                 0                    1   \n",
       "1                0                 0                    0   \n",
       "2                0                 0                    0   \n",
       "3                0                 0                    1   \n",
       "4                0                 0                    0   \n",
       "...            ...               ...                  ...   \n",
       "40195            0                 0                    1   \n",
       "40196            0                 0                    0   \n",
       "40197            0                 0                    3   \n",
       "40198            0                 0                    1   \n",
       "40199            0                 0                    0   \n",
       "\n",
       "       desperation_word_count  ...  murder_word_count  paranoia_word_count  \\\n",
       "0                           1  ...                  1                    0   \n",
       "1                           0  ...                  0                    0   \n",
       "2                           0  ...                  0                    0   \n",
       "3                           0  ...                  0                    0   \n",
       "4                           0  ...                  0                    0   \n",
       "...                       ...  ...                ...                  ...   \n",
       "40195                       0  ...                  0                    0   \n",
       "40196                       0  ...                  0                    0   \n",
       "40197                       2  ...                  0                    1   \n",
       "40198                       2  ...                  0                    0   \n",
       "40199                       1  ...                  1                    0   \n",
       "\n",
       "       planning_word_count  relationship_word_count  soldier_word_count  \\\n",
       "0                        0                        1                   0   \n",
       "1                        0                        0                   0   \n",
       "2                        0                        0                   0   \n",
       "3                        1                        1                   0   \n",
       "4                        0                        0                   0   \n",
       "...                    ...                      ...                 ...   \n",
       "40195                    1                        0                   0   \n",
       "40196                    0                        1                   0   \n",
       "40197                    1                        0                   0   \n",
       "40198                    1                        0                   0   \n",
       "40199                    0                        0                   1   \n",
       "\n",
       "       suicide_word_count  surveillance_word_count  threat_word_count  \\\n",
       "0                       1                        0                  0   \n",
       "1                       0                        0                  0   \n",
       "2                       0                        0                  0   \n",
       "3                       0                        0                  0   \n",
       "4                       0                        0                  0   \n",
       "...                   ...                      ...                ...   \n",
       "40195                   0                        0                  0   \n",
       "40196                   0                        0                  0   \n",
       "40197                   0                        1                  0   \n",
       "40198                   0                        0                  0   \n",
       "40199                   0                        0                  1   \n",
       "\n",
       "       violence_word_count  weaponry_word_count  \n",
       "0                        0                    0  \n",
       "1                        0                    0  \n",
       "2                        0                    0  \n",
       "3                        0                    0  \n",
       "4                        0                    0  \n",
       "...                    ...                  ...  \n",
       "40195                    0                    0  \n",
       "40196                    0                    0  \n",
       "40197                    0                    0  \n",
       "40198                    0                    0  \n",
       "40199                    1                    1  \n",
       "\n",
       "[40200 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grievance_set_dictionary: Dict[str, Counter] = defaultdict(Counter)\n",
    "for category in categorys:\n",
    "    curr_category_set = set(grievance_dict_df.loc[grievance_dict_df.category == category].word.to_list())\n",
    "    stemmed_comments, grievance_set_dictionary[category] = dictionary_analysis_over_set_intersection(dict_name=category, dict_set=curr_category_set, data=stemmed_comments)\n",
    "stemmed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa91c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.merge(comment_df, stemmed_comments[[\"id\", \"deadline_word_count\", \"desperation_word_count\", \"fixation_word_count\", 'frustration_word_count', 'god_word_count',\n",
    "       'grievance_word_count', 'hate_word_count', 'help_word_count', 'honour_word_count', 'impostor_word_count', 'jealousy_word_count',\n",
    "       'loneliness_word_count', 'murder_word_count', 'paranoia_word_count', 'planning_word_count', 'relationship_word_count',\n",
    "       'soldier_word_count', 'suicide_word_count', 'surveillance_word_count', 'threat_word_count', 'violence_word_count',\n",
    "       'weaponry_word_count']], on=\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae11a84a",
   "metadata": {},
   "source": [
    "### Ethnic Slurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76d288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Location or origin</th>\n",
       "      <th>Targets</th>\n",
       "      <th>Meaning, origin and notes</th>\n",
       "      <th>References</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eight ball, 8ball</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black people</td>\n",
       "      <td>Referring to the black ball in pool. Slang, us...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eyetie</td>\n",
       "      <td>United States, United Kingdom</td>\n",
       "      <td>Italian people</td>\n",
       "      <td>Originated through the mispronunciation of \"It...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dago, Dego</td>\n",
       "      <td>United States, Commonwealth</td>\n",
       "      <td>Italians, Spaniards, Portuguese people</td>\n",
       "      <td>Possibly derived from the Spanish name \"Diego\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dago, Dego</td>\n",
       "      <td>United States</td>\n",
       "      <td>Italian people</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dal Khor</td>\n",
       "      <td>Urdu-speaking people</td>\n",
       "      <td>Indians and Pakistanis (specifically Punjabis)</td>\n",
       "      <td>The term literally translates to \"dal eater\", ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Huinca</td>\n",
       "      <td>Argentina, Chile</td>\n",
       "      <td>Non-Mapuche Chileans, non-Mapuche Argentines</td>\n",
       "      <td>Mapuche term dating back at least to the Conqu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Hun</td>\n",
       "      <td>United States, United Kingdom</td>\n",
       "      <td>German people</td>\n",
       "      <td>(United States, United Kingdom) Germans, espec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Hun</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Protestants and British soldiers</td>\n",
       "      <td>A Protestant in Northern Ireland or historical...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>Hunky, Hunk</td>\n",
       "      <td>United States</td>\n",
       "      <td>Central European laborers.</td>\n",
       "      <td>It originated in the coal regions of Pennsylva...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Hymie</td>\n",
       "      <td>United States</td>\n",
       "      <td>Jewish people</td>\n",
       "      <td>Derived from the personal name Hyman (from the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Term             Location or origin  \\\n",
       "0    Eight ball, 8ball                            NaN   \n",
       "1               Eyetie  United States, United Kingdom   \n",
       "2           Dago, Dego    United States, Commonwealth   \n",
       "3           Dago, Dego                  United States   \n",
       "4             Dal Khor           Urdu-speaking people   \n",
       "..                 ...                            ...   \n",
       "424             Huinca               Argentina, Chile   \n",
       "425                Hun  United States, United Kingdom   \n",
       "426                Hun                        Ireland   \n",
       "427        Hunky, Hunk                  United States   \n",
       "428              Hymie                  United States   \n",
       "\n",
       "                                            Targets  \\\n",
       "0                                      Black people   \n",
       "1                                    Italian people   \n",
       "2            Italians, Spaniards, Portuguese people   \n",
       "3                                    Italian people   \n",
       "4    Indians and Pakistanis (specifically Punjabis)   \n",
       "..                                              ...   \n",
       "424    Non-Mapuche Chileans, non-Mapuche Argentines   \n",
       "425                                   German people   \n",
       "426                Protestants and British soldiers   \n",
       "427                      Central European laborers.   \n",
       "428                                   Jewish people   \n",
       "\n",
       "                             Meaning, origin and notes References  \n",
       "0    Referring to the black ball in pool. Slang, us...        NaN  \n",
       "1    Originated through the mispronunciation of \"It...        NaN  \n",
       "2       Possibly derived from the Spanish name \"Diego\"        NaN  \n",
       "3                                                  NaN        NaN  \n",
       "4    The term literally translates to \"dal eater\", ...        NaN  \n",
       "..                                                 ...        ...  \n",
       "424  Mapuche term dating back at least to the Conqu...        NaN  \n",
       "425  (United States, United Kingdom) Germans, espec...        NaN  \n",
       "426  A Protestant in Northern Ireland or historical...        NaN  \n",
       "427  It originated in the coal regions of Pennsylva...        NaN  \n",
       "428  Derived from the personal name Hyman (from the...        NaN  \n",
       "\n",
       "[429 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "import os.path\n",
    "\n",
    "dict_files: list = list(filter(lambda f: f[-4:] == \".csv\" ,listdir(\"dictionaries/ethnic_slurs/\")))\n",
    "dict_df: pd.DataFrame = pd.DataFrame()\n",
    "for file in dict_files:\n",
    "    part = pd.read_csv(os.path.join(\"dictionaries/ethnic_slurs\", file))\n",
    "    dict_df = pd.concat([part, dict_df])\n",
    "dict_df.reset_index(inplace=True, drop=True)\n",
    "ethnic_slurs_set: set = set(dict_df.Term.to_list())\n",
    "dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e425e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>toxic_word_count</th>\n",
       "      <th>deadline_word_count</th>\n",
       "      <th>desperation_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>paranoia_word_count</th>\n",
       "      <th>planning_word_count</th>\n",
       "      <th>relationship_word_count</th>\n",
       "      <th>soldier_word_count</th>\n",
       "      <th>suicide_word_count</th>\n",
       "      <th>surveillance_word_count</th>\n",
       "      <th>threat_word_count</th>\n",
       "      <th>violence_word_count</th>\n",
       "      <th>weaponry_word_count</th>\n",
       "      <th>ethnic_slurs_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>lX0rXb7gqrU</td>\n",
       "      <td>Ugz1sXMTvNTUYAw0HsN4AaABAg</td>\n",
       "      <td>Mick&amp;#39;s little &amp;quot;RAWR&amp;quot; is absolute...</td>\n",
       "      <td>UCjqGkDwAUy7GHWjAXZ6q-fw</td>\n",
       "      <td>1684</td>\n",
       "      <td>2022-08-06T22:03:15Z</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>lX0rXb7gqrU</td>\n",
       "      <td>Ugw1cZC76106B9-vnUJ4AaABAg</td>\n",
       "      <td>I’m dying at Mick’s “rawr” when he shows the d...</td>\n",
       "      <td>UCIYRowSxRLrYej_QuHBpmDQ</td>\n",
       "      <td>140</td>\n",
       "      <td>2022-08-07T08:46:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Laz6i7970Ys</td>\n",
       "      <td>UgwX6btkQoGJUbG9yl54AaABAg.9LiQYA3sbPR9LtCzn1_t3J</td>\n",
       "      <td>Mick&amp;#39;s is also from the paddock.</td>\n",
       "      <td>UC7_9Ynd1sRJXGpaJ-pW6smw</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-04-08T19:48:08Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>m8tGGT-N9AA</td>\n",
       "      <td>UgxtvD1tZSrqV4t4Gel4AaABAg.9e1aS1QCjPI9e2B_gnrjfM</td>\n",
       "      <td>@António Taveira gentleman? He drove into Hami...</td>\n",
       "      <td>UCLLM_9awwydQCOf3kGIwVGA</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-28T22:44:43Z</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>I1WEmbI12H4</td>\n",
       "      <td>UgzGiDkS5UhFiWKkCRV4AaABAg.9Ao68erlL6N9AoMXqEVYaW</td>\n",
       "      <td>Apple Man yeah but he wasn’t on the podium thi...</td>\n",
       "      <td>UCec3onObanftQ7JAURFY_BQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-07T12:50:01Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50254</th>\n",
       "      <td>iaicuH7q248</td>\n",
       "      <td>UgxTlNSAGQNyo_-x-SJ4AaABAg.9dlCMJ9x2aW9dmBTk8ZpS9</td>\n",
       "      <td>@Charlie mccahill When&amp;#39;s the next upgrade ...</td>\n",
       "      <td>UC5aaxPMB7S5XWFN1Vnx8q9g</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-22T08:16:41Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50255</th>\n",
       "      <td>iaicuH7q248</td>\n",
       "      <td>UgxTlNSAGQNyo_-x-SJ4AaABAg.9dlCMJ9x2aW9dmBTk8ZpS9</td>\n",
       "      <td>@Charlie mccahill When&amp;#39;s the next upgrade ...</td>\n",
       "      <td>UC5aaxPMB7S5XWFN1Vnx8q9g</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-22T08:16:41Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51562</th>\n",
       "      <td>lsdqnsre1Ik</td>\n",
       "      <td>UgxV81WeTY0dy3kSqet4AaABAg.9BuLmtwSMCO9BwDy5BblH3</td>\n",
       "      <td>If Mick Schumacher got 4th he would be there i...</td>\n",
       "      <td>UC11AwLSHfnhT-Va2tTRs47Q</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-08-04T10:40:25Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52811</th>\n",
       "      <td>SbDc8oAP2LE</td>\n",
       "      <td>UgxXlnYbYFenP8H5j5B4AaABAg</td>\n",
       "      <td>Mick’s interview is basically just a floating ...</td>\n",
       "      <td>UCGKGiQex5tU4HzJTgBNSZjw</td>\n",
       "      <td>813</td>\n",
       "      <td>2022-04-07T19:17:08Z</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53895</th>\n",
       "      <td>efZPHk_lln4</td>\n",
       "      <td>UgxD_u3SFguxudmulxF4AaABAg</td>\n",
       "      <td>Mick‘s brief confusion after his interview is ...</td>\n",
       "      <td>UCpz-4hCIaZ8KlTtR-wmp-Pw</td>\n",
       "      <td>289</td>\n",
       "      <td>2021-05-02T16:53:23Z</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                                 id  \\\n",
       "551    lX0rXb7gqrU                         Ugz1sXMTvNTUYAw0HsN4AaABAg   \n",
       "595    lX0rXb7gqrU                         Ugw1cZC76106B9-vnUJ4AaABAg   \n",
       "1179   Laz6i7970Ys  UgwX6btkQoGJUbG9yl54AaABAg.9LiQYA3sbPR9LtCzn1_t3J   \n",
       "3370   m8tGGT-N9AA  UgxtvD1tZSrqV4t4Gel4AaABAg.9e1aS1QCjPI9e2B_gnrjfM   \n",
       "5289   I1WEmbI12H4  UgzGiDkS5UhFiWKkCRV4AaABAg.9Ao68erlL6N9AoMXqEVYaW   \n",
       "...            ...                                                ...   \n",
       "50254  iaicuH7q248  UgxTlNSAGQNyo_-x-SJ4AaABAg.9dlCMJ9x2aW9dmBTk8ZpS9   \n",
       "50255  iaicuH7q248  UgxTlNSAGQNyo_-x-SJ4AaABAg.9dlCMJ9x2aW9dmBTk8ZpS9   \n",
       "51562  lsdqnsre1Ik  UgxV81WeTY0dy3kSqet4AaABAg.9BuLmtwSMCO9BwDy5BblH3   \n",
       "52811  SbDc8oAP2LE                         UgxXlnYbYFenP8H5j5B4AaABAg   \n",
       "53895  efZPHk_lln4                         UgxD_u3SFguxudmulxF4AaABAg   \n",
       "\n",
       "                                                    text  \\\n",
       "551    Mick&#39;s little &quot;RAWR&quot; is absolute...   \n",
       "595    I’m dying at Mick’s “rawr” when he shows the d...   \n",
       "1179                Mick&#39;s is also from the paddock.   \n",
       "3370   @António Taveira gentleman? He drove into Hami...   \n",
       "5289   Apple Man yeah but he wasn’t on the podium thi...   \n",
       "...                                                  ...   \n",
       "50254  @Charlie mccahill When&#39;s the next upgrade ...   \n",
       "50255  @Charlie mccahill When&#39;s the next upgrade ...   \n",
       "51562  If Mick Schumacher got 4th he would be there i...   \n",
       "52811  Mick’s interview is basically just a floating ...   \n",
       "53895  Mick‘s brief confusion after his interview is ...   \n",
       "\n",
       "                           user  like_count          published_at  \\\n",
       "551    UCjqGkDwAUy7GHWjAXZ6q-fw        1684  2022-08-06T22:03:15Z   \n",
       "595    UCIYRowSxRLrYej_QuHBpmDQ         140  2022-08-07T08:46:47Z   \n",
       "1179   UC7_9Ynd1sRJXGpaJ-pW6smw           2  2021-04-08T19:48:08Z   \n",
       "3370   UCLLM_9awwydQCOf3kGIwVGA           0  2022-07-28T22:44:43Z   \n",
       "5289   UCec3onObanftQ7JAURFY_BQ           0  2020-07-07T12:50:01Z   \n",
       "...                         ...         ...                   ...   \n",
       "50254  UC5aaxPMB7S5XWFN1Vnx8q9g           2  2022-07-22T08:16:41Z   \n",
       "50255  UC5aaxPMB7S5XWFN1Vnx8q9g           2  2022-07-22T08:16:41Z   \n",
       "51562  UC11AwLSHfnhT-Va2tTRs47Q           6  2020-08-04T10:40:25Z   \n",
       "52811  UCGKGiQex5tU4HzJTgBNSZjw         813  2022-04-07T19:17:08Z   \n",
       "53895  UCpz-4hCIaZ8KlTtR-wmp-Pw         289  2021-05-02T16:53:23Z   \n",
       "\n",
       "       reply_count  toxic_word_count  deadline_word_count  \\\n",
       "551              4                 0                    1   \n",
       "595              0                 0                    0   \n",
       "1179             0                 0                    0   \n",
       "3370             0                 1                    1   \n",
       "5289             0                 0                    1   \n",
       "...            ...               ...                  ...   \n",
       "50254            0                 0                    1   \n",
       "50255            0                 0                    1   \n",
       "51562            0                 0                    0   \n",
       "52811           19                 0                    0   \n",
       "53895            3                 0                    0   \n",
       "\n",
       "       desperation_word_count  ...  paranoia_word_count  planning_word_count  \\\n",
       "551                         0  ...                    0                    1   \n",
       "595                         0  ...                    1                    0   \n",
       "1179                        0  ...                    0                    0   \n",
       "3370                        1  ...                    1                    0   \n",
       "5289                        0  ...                    1                    0   \n",
       "...                       ...  ...                  ...                  ...   \n",
       "50254                       0  ...                    0                    1   \n",
       "50255                       0  ...                    0                    1   \n",
       "51562                       0  ...                    0                    0   \n",
       "52811                       0  ...                    0                    1   \n",
       "53895                       1  ...                    1                    1   \n",
       "\n",
       "       relationship_word_count  soldier_word_count  suicide_word_count  \\\n",
       "551                          0                   0                   0   \n",
       "595                          0                   1                   1   \n",
       "1179                         0                   0                   0   \n",
       "3370                         2                   1                   1   \n",
       "5289                         1                   1                   0   \n",
       "...                        ...                 ...                 ...   \n",
       "50254                        0                   0                   0   \n",
       "50255                        0                   0                   0   \n",
       "51562                        0                   0                   0   \n",
       "52811                        0                   0                   0   \n",
       "53895                        2                   0                   0   \n",
       "\n",
       "       surveillance_word_count  threat_word_count  violence_word_count  \\\n",
       "551                          0                  0                    0   \n",
       "595                          1                  0                    1   \n",
       "1179                         0                  0                    0   \n",
       "3370                         1                  1                    2   \n",
       "5289                         0                  0                    1   \n",
       "...                        ...                ...                  ...   \n",
       "50254                        0                  0                    0   \n",
       "50255                        0                  0                    0   \n",
       "51562                        1                  0                    0   \n",
       "52811                        0                  0                    0   \n",
       "53895                        0                  0                    0   \n",
       "\n",
       "       weaponry_word_count  ethnic_slurs_word_count  \n",
       "551                      1                        1  \n",
       "595                      1                        1  \n",
       "1179                     0                        1  \n",
       "3370                     0                        1  \n",
       "5289                     1                        1  \n",
       "...                    ...                      ...  \n",
       "50254                    0                        1  \n",
       "50255                    0                        1  \n",
       "51562                    0                        1  \n",
       "52811                    0                        1  \n",
       "53895                    0                        1  \n",
       "\n",
       "[96 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df, ethnic_slurs_counter = dictionary_analysis_over_set_intersection(dict_name=\"ethnic_slurs\", dict_set=ethnic_slurs_set, data=comment_df)\n",
    "comment_df.loc[comment_df[\"ethnic_slurs_word_count\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c931cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Mick': 50,\n",
       "         'Charlie': 16,\n",
       "         'Apple': 2,\n",
       "         'Monkey': 10,\n",
       "         'Banana': 7,\n",
       "         'Yank': 1,\n",
       "         'Flip': 1,\n",
       "         'Yellow': 5,\n",
       "         'Turk': 4})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnic_slurs_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c886136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.to_csv(\"datasets/comment_df_dicts.csv\" ,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beccab78",
   "metadata": {},
   "source": [
    "## Transformer Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad631faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_id                                              uRPUm8pcYoQ\n",
      "id              UgwvCzQ7192Kg4xHGfR4AaABAg.9_3E7nPClyU9dxpHFypTCQ\n",
      "text            @Agnt 1214 yqyyqqqqqyyuqyqyqqqqqwquuqqqququqqu...\n",
      "user                                     UCM0MiIJ_Q6Xs0NpiFp1M4cQ\n",
      "like_count                                                      0\n",
      "published_at                                 2022-07-26T20:44:53Z\n",
      "reply_count                                                     0\n",
      "Name: 14603, dtype: object\n",
      "video_id                                              uRPUm8pcYoQ\n",
      "id              UgwvCzQ7192Kg4xHGfR4AaABAg.9_3E7nPClyU9_4n23nz24U\n",
      "text                             James hunt vibes from hulkenburg\n",
      "user                                     UC4T7J9F-QzSPz5ogXYV5Acw\n",
      "like_count                                                      0\n",
      "published_at                                 2022-03-27T16:22:17Z\n",
      "reply_count                                                     0\n",
      "Name: 14604, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10952ca3acc47b8b813a5a3ec8154a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/40197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>reply_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg</td>\n",
       "      <td>THAT SUPER MAX AT THE END WAS FANTASTIC</td>\n",
       "      <td>UCmCBpDZeM9LbTeWbMPW1N2A</td>\n",
       "      <td>4626</td>\n",
       "      <td>2021-12-03T19:48:05Z</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD</td>\n",
       "      <td>@user I got cold</td>\n",
       "      <td>UCQtwk7iBtv0hTwedp13yDKA</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-02T08:40:19Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9</td>\n",
       "      <td>@user is Awesome oh yes.</td>\n",
       "      <td>UCuzQCuC86Pmhq9ixSff1z-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-01T18:23:44Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu</td>\n",
       "      <td>Yea that is a fact, but if i would write like ...</td>\n",
       "      <td>UCgFRK81ZtgryEHR6GAxbWvQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-28T15:47:42Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FZjG3oft5rs</td>\n",
       "      <td>UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW</td>\n",
       "      <td>@user is Awesome didn’t age to well ehh?</td>\n",
       "      <td>UCADfAgrwLT4ZzpGkC8L-4gw</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-28T02:08:51Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40195</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgwZ6iauaq1cULpaUvR4AaABAg</td>\n",
       "      <td>How you managed Juncao is so amazing. That has...</td>\n",
       "      <td>UCmHLqIoiFsxOwsIEsqAVgzQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-16T11:01:58Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40196</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugxb7TesuwCHGhDxZRJ4AaABAg</td>\n",
       "      <td>Absolute brilliance from Seb on the radio as a...</td>\n",
       "      <td>UCalYkEFZn7f91L316msDu7Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-25T16:01:06Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40197</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>Ugx4RZlmzo8UHnkajsB4AaABAg</td>\n",
       "      <td>I'll come back from time to time to watch thes...</td>\n",
       "      <td>UCYDZKuIlaaGAR-BOvs7W63Q</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T01:40:42Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40198</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgxbU_9pwAhxAIQkcRp4AaABAg</td>\n",
       "      <td>Going to miss Seb!</td>\n",
       "      <td>UCd4Bo6BxK0PHQnukaxeqnZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-10T21:19:20Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40199</th>\n",
       "      <td>g5yQmp1ctXk</td>\n",
       "      <td>UgyKB7N5-weM_DcpKbt4AaABAg</td>\n",
       "      <td>Ferrari Seb 2015-2018 nice but than how he fig...</td>\n",
       "      <td>UCylv5dmN-I2r7WxT2090rhg</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-19T20:23:04Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40197 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                                 id  \\\n",
       "0      FZjG3oft5rs                         UgznjAR9SoXoE0gpoK54AaABAg   \n",
       "1      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9eDYvXWpXVD   \n",
       "2      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9Xuy0JALbx9   \n",
       "3      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WVZKoNgEnu   \n",
       "4      FZjG3oft5rs  UgznjAR9SoXoE0gpoK54AaABAg.9VVbyYu52Uy9WU5cOYUMFW   \n",
       "...            ...                                                ...   \n",
       "40195  g5yQmp1ctXk                         UgwZ6iauaq1cULpaUvR4AaABAg   \n",
       "40196  g5yQmp1ctXk                         Ugxb7TesuwCHGhDxZRJ4AaABAg   \n",
       "40197  g5yQmp1ctXk                         Ugx4RZlmzo8UHnkajsB4AaABAg   \n",
       "40198  g5yQmp1ctXk                         UgxbU_9pwAhxAIQkcRp4AaABAg   \n",
       "40199  g5yQmp1ctXk                         UgyKB7N5-weM_DcpKbt4AaABAg   \n",
       "\n",
       "                                                    text  \\\n",
       "0                THAT SUPER MAX AT THE END WAS FANTASTIC   \n",
       "1                                       @user I got cold   \n",
       "2                               @user is Awesome oh yes.   \n",
       "3      Yea that is a fact, but if i would write like ...   \n",
       "4               @user is Awesome didn’t age to well ehh?   \n",
       "...                                                  ...   \n",
       "40195  How you managed Juncao is so amazing. That has...   \n",
       "40196  Absolute brilliance from Seb on the radio as a...   \n",
       "40197  I'll come back from time to time to watch thes...   \n",
       "40198                                 Going to miss Seb!   \n",
       "40199  Ferrari Seb 2015-2018 nice but than how he fig...   \n",
       "\n",
       "                           user  like_count          published_at  reply_count  \n",
       "0      UCmCBpDZeM9LbTeWbMPW1N2A        4626  2021-12-03T19:48:05Z           48  \n",
       "1      UCQtwk7iBtv0hTwedp13yDKA           0  2022-08-02T08:40:19Z            0  \n",
       "2      UCuzQCuC86Pmhq9ixSff1z-A           0  2022-02-01T18:23:44Z            0  \n",
       "3      UCgFRK81ZtgryEHR6GAxbWvQ           1  2021-12-28T15:47:42Z            0  \n",
       "4      UCADfAgrwLT4ZzpGkC8L-4gw           0  2021-12-28T02:08:51Z            0  \n",
       "...                         ...         ...                   ...          ...  \n",
       "40195  UCmHLqIoiFsxOwsIEsqAVgzQ           0  2022-11-16T11:01:58Z            0  \n",
       "40196  UCalYkEFZn7f91L316msDu7Q           0  2022-11-25T16:01:06Z            0  \n",
       "40197  UCYDZKuIlaaGAR-BOvs7W63Q           1  2022-11-10T01:40:42Z            0  \n",
       "40198  UCd4Bo6BxK0PHQnukaxeqnZg           1  2022-11-10T21:19:20Z            0  \n",
       "40199  UCylv5dmN-I2r7WxT2090rhg           0  2022-12-19T20:23:04Z            0  \n",
       "\n",
       "[40197 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import swifter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Delete unreadable comments that result in the models crashing\n",
    "print(comment_df.iloc[14603])\n",
    "comment_df.drop([14603], inplace=True) # comment is full of random characters\n",
    "comment_df.drop([27224, 27223], inplace=True) # comment is not in english\n",
    "comment_df.reset_index(drop=True)\n",
    "print(comment_df.iloc[14603])\n",
    "\n",
    "comment_df_for_transformers = comment_df.copy()\n",
    "comment_df_for_transformers[\"text\"] = comment_df_for_transformers.text.swifter.apply(lambda text: preprocess(text))\n",
    "comment_df_for_transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18617cb8",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d356d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 68%|██████▊   | 27223/40198 [21:10<10:05, 21.43it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/I516998/Documents/uni - mannheim/Computational Analysis of Communications/Final/final.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I516998/Documents/uni%20-%20mannheim/Computational%20Analysis%20of%20Communications/Final/final.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m tqdm(comment_df_for_transformers\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mto_list()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I516998/Documents/uni%20-%20mannheim/Computational%20Analysis%20of%20Communications/Final/final.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     encoded_input \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m514\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/I516998/Documents/uni%20-%20mannheim/Computational%20Analysis%20of%20Communications/Final/final.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I516998/Documents/uni%20-%20mannheim/Computational%20Analysis%20of%20Communications/Final/final.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     scores \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/I516998/Documents/uni%20-%20mannheim/Computational%20Analysis%20of%20Communications/Final/final.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     scores \u001b[39m=\u001b[39m softmax(scores)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1215\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1215\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1216\u001b[0m     input_ids,\n\u001b[1;32m   1217\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1218\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1219\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1220\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1221\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1222\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1223\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1224\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1225\u001b[0m )\n\u001b[1;32m   1226\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1227\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:846\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    844\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 846\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    847\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    848\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    849\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    850\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    851\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    852\u001b[0m )\n\u001b[1;32m    853\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    854\u001b[0m     embedding_output,\n\u001b[1;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    863\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    864\u001b[0m )\n\u001b[1;32m    865\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:129\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    127\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embeddings(position_ids)\n\u001b[1;32m    130\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[1;32m    131\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/CAC/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "label_list = []\n",
    "score_list = []\n",
    "\n",
    "for text in tqdm(comment_df_for_transformers.text.to_list()):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=514)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    label_list.append(config.id2label[np.argsort(scores)[::-1][0]])\n",
    "    score_list.append(max(scores))\n",
    "\n",
    "comment_df[\"sentiment\"] = label_list\n",
    "comment_df[\"sentiment_score\"] = score_list\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.to_csv(\"datasets/comment_df_sentiment_transformer.csv\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e7bd991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user Yo recuerdo a Hamilton terminando la temporada con los mismos puntos que Alonso. Yo recuerdo a Hamilton, que la única carrera que no fue competitivo, es porque como el mismo había dicho ¨ usé unos reglajes distintos a los de Alonso ¨. Yo recuerdo a Ron Denis por ahí a mitad de temporada diciendo ¨corremos contra Alonso¨. Yo recuerdo a Hamilton, saliéndose de pista y la grúa lo volvió a meter en la carrera, cosa que no hizo con el resto de coches. Yo recuerdo a Nolbert Hauk diciéndole a Alonso ¨sabes que poco cuesta hacer 2 decimas el coche mas lento ? ¨. Yo recuerdo a Alonso, que de repente, de luchar por las poles, empezó a quedar sistemáticamente 2 o 3 decimas por detrás de Hamilton en Qualy. Yo recuerdo muchas cosas de ese año, pero que Hamilton ganase a Alonso, eso solo lo diría un fanático incapaz de ver con objetividad.Alonso, en un equipo el cual le fue completamente hostil, y que le hizo el coche mas lento a propósito, y el cual lo usó para darle todo el apoyo a Hamilton que necesitaba sobre todo con los setups del coche, y lo máximo que consiguió, fue empatar a puntos con Alonso.Por cierto, no te diste cuenta en la carrera de Mónaco como Alonso ralentizó el ritmo ? Recuerdo a Hamilton y toda la prensa británica llorando porque supuestamente no le dejaron adelantar. Pues bien, Alonso demostró en Monaco ese dia, que ni yendo 3 segundos por vuelta mas lento, y no habiendo ordenes de equipo porque corren en equipos diferentes, no tiene ni la mas remota posibilidad de adelantarlo. O lo que es lo mismo, demostró que Hamilton es un LLORÓN.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df_for_transformers.iloc[27223].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7532b47b",
   "metadata": {},
   "source": [
    "### Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8251d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40197 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7369702 -0.8776053]\n",
      "['NON_HATE'] [0.8340457]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "label_list = []\n",
    "score_list = []\n",
    "\n",
    "for text in tqdm(comment_df_for_transformers.text.to_list()):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=514)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    label_list.append(config.id2label[np.argsort(scores)[::-1][0]])\n",
    "    score_list.append(max(scores))\n",
    "\n",
    "comment_df[\"sentiment\"] = label_list\n",
    "comment_df[\"sentiment_score\"] = score_list\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ab9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.to_csv(\"datasets/comment_df_hate_speech_transformer.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3535ee24",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a8f994c",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb13c4",
   "metadata": {},
   "source": [
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed7f3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook final.ipynb to markdown\n",
      "[NbConvertApp] Writing 25078 bytes to final.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"jupyter nbconvert --to markdown final.ipynb\")\n",
    "os.system(\"pandoc -s final.md -t pdf -o final.pdf --citeproc --bibliography=refs.bib --csl=apa.csl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641d154",
   "metadata": {},
   "source": [
    "-->"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "13409951/AYIUUYSI": {
      "DOI": "10.3758/s13428-021-01536-2",
      "URL": "https://link.springer.com/10.3758/s13428-021-01536-2",
      "abstract": "This paper introduces the Grievance Dictionary, a psycholinguistic dictionary that can be used to automatically understand language use in the context of grievance-fueled violence threat assessment. We describe the development of the dictionary, which was informed by suggestions from experienced threat assessment practitioners. These suggestions and subsequent human and computational word list generation resulted in a dictionary of 20,502 words annotated by 2318 participants. The dictionary was validated by applying it to texts written by violent and non-violent individuals, showing strong evidence for a difference between populations in several dictionary categories. Further classification tasks showed promising performance, but future improvements are still needed. Finally, we provide instructions and suggestions for the use of the Grievance Dictionary by security professionals and (violence) researchers.",
      "accessed": {
       "date-parts": [
        [
         2023,
         1,
         7
        ]
       ]
      },
      "author": [
       {
        "family": "van der Vegt",
        "given": "Isabelle"
       },
       {
        "family": "Mozes",
        "given": "Maximilian"
       },
       {
        "family": "Kleinberg",
        "given": "Bennett"
       },
       {
        "family": "Gill",
        "given": "Paul"
       }
      ],
      "container-title": "Behavior Research Methods",
      "id": "13409951/AYIUUYSI",
      "issue": "5",
      "issued": {
       "date-parts": [
        [
         "2021"
        ]
       ]
      },
      "journalAbbreviation": "Behav Res",
      "language": "en",
      "page": "2105-2119",
      "shortTitle": "The Grievance Dictionary",
      "system_id": "zotero|13409951/AYIUUYSI",
      "title": "The Grievance Dictionary: Understanding threatening language use",
      "type": "article-journal",
      "volume": "53"
     },
     "13409951/K57UGHVY": {
      "URL": "https://github.com/Orthrus-Lexicon/Toxic",
      "abstract": "A dictionary of toxic words",
      "accessed": {
       "date-parts": [
        [
         2023,
         1,
         7
        ]
       ]
      },
      "author": [
       {
        "family": "Orthrus-Lexicon",
        "given": ""
       }
      ],
      "id": "13409951/K57UGHVY",
      "issued": {
       "date-parts": [
        [
         2022,
         9,
         9
        ]
       ]
      },
      "note": "original-date: 2021-02-14T15:47:49Z",
      "system_id": "zotero|13409951/K57UGHVY",
      "title": "Orthrus Toxic Dictionary implementation",
      "type": "book"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('CAC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9841eda7af3f17386775c2d69f63acccb0fbca7caaddff7551f32402eb39582"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
