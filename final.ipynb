{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f04061f",
   "metadata": {},
   "source": [
    "<!--"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3854acad-dc94-4dd7-b65e-e336e57f2428",
   "metadata": {
    "citation-manager": {
     "citations": {
      "6d9q5": [
       {
        "id": "13409951/AYIUUYSI",
        "source": "zotero"
       }
      ],
      "6pejh": [
       {
        "id": "13409951/K57UGHVY",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "# Final Report - Work in Progress\n",
    "- Research Hypothesis / Questions:\n",
    "    - Is Formula 1 fandom Toxic?\n",
    "    - Are there specific groups that show more toxic behaviour then others?\n",
    "    - Is the toxicity a \"self-made\" problem of Formula 1?\n",
    "- APIs: Youtube\n",
    "    - (Not reddit as post are often off topic especially during the off season, that we are currently in)\n",
    "- Methods:\n",
    "    - TBD\n",
    "    - Dictionary\n",
    "        - Formula 1 specific words that are toxic\n",
    "        - racism / ethnic slurs -> [@ethnic_slurs]\n",
    "        - toxicity -> [@orthrus-lexicon_orthrus_2022]\n",
    "        - hate speech -> [@van_der_vegt_grievance_2021]\n",
    "        - insults -> [@van_der_vegt_grievance_2021]\n",
    "    - Transformer classifier\n",
    "        - sentiment -> cardiffnlp/twitter-roberta-base-sentiment-latest [@tweet_sentiment_classifier]\n",
    "        - racism -> jaumefib/datathon-against-racism [@hate_speech_classifier]\n",
    "        - hate speech -> Hate-speech-CNERG/dehatebert-mono-english [@racism_classifier]\n",
    "    - statistical analysis\n",
    "        - group toxic behavior by drivers and teams\n",
    "        - group by topics\n",
    "            - topic modelling?\n",
    "- Contents:\n",
    "    - Introduction\n",
    "        - What is Formula 1\n",
    "        - Why do we need to analyze this\n",
    "        - introduce the three research questions / hypothesis\n",
    "    - Fundamentals\n",
    "        - Formula 1\n",
    "        - What is fandom\n",
    "          - \n",
    "        - Defining toxic fan behavior\n",
    "        - Youtube API\n",
    "        - Maybe explaining the used methods?\n",
    "    - Concept\n",
    "        - What will be done\n",
    "        - How will i be doing it\n",
    "    - Creating the Dataset\n",
    "        - Explain Dataset creation\n",
    "    - Applying Method 1\n",
    "    - Applying Method 2\n",
    "    - Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0175e8b5",
   "metadata": {},
   "source": [
    "-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db77bb04",
   "metadata": {},
   "source": [
    "# Analysing Toxicity in Formula 1 Fandom - Computational Analysis of Communications Final\n",
    "Author: Leon Knorr\n",
    "\n",
    "Matr-Nr: 1902854"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d9e388",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "In order to use Citations in Jupyter Notebook, the whole Notebook has to be converted to markdown and after that, the markdown file has to be compiled with LATEX and the bibliography and bibliography style is injected. Because of that Citations and the bibliography are only visible in the PDF version of the notebook. However because comments contain emojis, and other special characters, the output of each code cell has to be cleared before the notebook is converted otherwise the pdf compile will fail. In addition to that the formating of the code cells in the pdf document is not necessarily perfect. As a result, Citations and bibliography will only be correctly visible in the PDF version, where as code and its output is only visible in the notebook source."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a5bd92",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Formula 1 is the highest class of international racing for open-wheel single-seater formula racing cars and is generally considered the most competitive, fastest and hardest class of motor racing. Since it’s first season in 1950, Formula 1 is visiting a diverse list of many different countries, where the best drivers in the world are racing against each other in teams of two drivers to determine the best driver and the best team on the Formula 1 grid [@about_f1]. These events are visited by thousands of Fans, with millions more following them on television and social media. With the 2021 season being one of the closest and most entertaining seasons in the history of Formula 1, where Red Bulls Max Verstappen beat Mercedes driver Lewis Hamilton in the grand finale of the season under controversial circumstances after a full season of controversy, drama and intense on track battles and with the release of Netflix Drive To Survive, Formula 1s popularity is growing rapidly. But, reports of Toxic and abusive Fan behavior at events and in comment sections on social media are accumulating, and casts an ugly shadow over Formula 1s latest successes [@woodhouse_scary_2022].\n",
    "As the reports over toxic and abusive fan behaviours in social media and at live events are rising, Formula 1 as well as Fans and drivers are taking a stand against toxicity in the Formula 1 community. However, an independent and scientific analysis of this topic is missing and therefore the accusations are sort of hanging in the air without a solid scientific foundation. Therefore, in order to tackle this problem research into the toxicity of Formula 1 fandom is a necassety to gain valuable insights into understanding the problem, where it originates from and to build a foundation for future measures to make attending Formula 1 events as well as the media around it a safer and more enjoyable experience. To take the first step into this direction, this thesis will analyse Youtube comments of the Formula 1 channel in order to determine:\n",
    "\n",
    "- If the Formula 1 fandom is toxic\n",
    "- Are there specific groups that are more toxic then others?\n",
    "- Is the toxicity a \"self-made\" problem of Formula 1 and where is the toxicity originating from?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edfdb523",
   "metadata": {},
   "source": [
    "## Fundamentals\n",
    "In this chapter the necessary fundamental knowledge is presented.\n",
    "\n",
    "### Formula 1\n",
    "Formula 1 is the worlds most prestigous motor racing competition, as well as the world's most popular annual sporting series [@about_f1]. It marks the highest class of international open-wheel single-seater formula racing. The first Formula 1 competition was held in 1950, since then the competiton for the world drivers championship (wdc) which determines the worlds best driver and the world constructors championship (wcc) which determines the best team, is held annualy and is sanctioned by the Fédération Internationale de l'Automobile (FIA). During the competition (also called a season), Formula 1 visits a variety of different countries and racing tracks, each event (Grands Prix) is attended by thousands of people with millions watching from home [@formula_1_2023]. All rights of the Formula 1 brand and the competition itself is owned by Formula One World Championship Limited, which is a corporation, that provides media distribution and promotion services, besides that, it controls the contracts, distribtution, and commercial management of rights and licenses of formula 1 [@formula_1_limited_company_profile]. The term Formula 1 is used to describe the corporation, as well as the competition, as they can't exist without each other.\n",
    "\n",
    "### What is Fandom\n",
    "According to Cornel Sandvoss Fandom is a community of people that are regularly, consuming a given popular narrative or text with great emotional involvement [@toxic_fandom]. The members of the community are called fans, which is a short form of \"fanatic\" [@arouh_toxic_2020]. In other words, a fandom is a community of people that are fanatic about a popular narrative or text such as a tv series, movie franchise or sports.\n",
    "\n",
    "Becoming a fan starts with the adoption of a fan identity about a fan object, thus fandom can be a powerful of defining the self. The fan object can be anything that people can be fanatic about, this may be a simple object such as trains or a virtual asset such as a movie franchise. Therefore, by taking part in a fandom, people are expressing themselfs through an identity they've chosen for themselfs. As a result, fans may lead to see the fan object as an extension of themselfs and thus react personally threatened if the fan object is facing a threat such as accusations etc [@toxic_fandom]. In addition to creating a strong part of their own identity, fans feel more connected or socialised through their fandom, as studies indicate, that even if fans don't interact with other members of a fan community, they still perceive themselfs as part of that community. Because of that, fans not only become personally invested in their fandom, they become socially invested as well [@toxic_fandom].\n",
    "\n",
    "As a result of the strong connection fans build up to their fan object, the time-frame in which this self identity has been chosen is also playing a role. As an example, many people build a fandom in their childhood about a tv series, franchise or sport, this often leads to them feeling entitled to having their fan object preserved as they deem acceptable. This behaviour is also called fan entitlement. A good example for this behaviour are the news movies and series in the Lord of the Rings and Star Wars franchises, as most fan communities of these franchises have been outraged about the new characters and story lines, where many people claimed that this \"ruined their childhood\" [@toxic_fandom].\n",
    "\n",
    "From an economic point of view, fandom and fan cultures are seen as the ideal costumers. They are eager to get their hands on the newest products and they are stable with re-occuring purchases, since intense consumption is considered a part of the fan identity [@arouh_toxic_2020].\n",
    "\n",
    "### Defining Toxic Fan behaviour\n",
    "In the first place, toxic fandom is a buzzword, that is widely used throughout media to describe or identify fans who engage in behaviors that are considered negative or unaccaptable. This behavior can range from simple negative responses to bullying other members of a fandom or those involved in the creation of the fan object [@toxic_fandom]. Most of this behaviour can be observed online in social media, there are however reports of toxic behaviour in real-life as well, such as abusive behaviour at events.\n",
    "\n",
    "The word toxic itself however is defined as \"of relating to, or caused by a toxin,\" \"of the nature of a poison; poisonous\" [@arouh_toxic_2020]. This definition originally originates from medival latin, where it refers to poisoned arrows or to being imbued with poison. Following this definition, it is an *external* substance that is toxic and not a person or their behaviour. However in recent years the understanding of this definition has shifted, today someones actions or the emotions experienced or types of character are now understood as poisonous or \"toxic\" [@arouh_toxic_2020]. This definition is closely related to the definition of the word fan, as explained earlier, fan originates from fanatic, which is traditionally linked to madness and demonic posession. This traditional and long obselete link is often exploited by media outlets to mark fans as psychopaths whose frustrated fantasies of intimate relationships or unsatisfied desires with the fan object take violent and ant-social forms [@arouh_toxic_2020]. In order to maintain this hypothesis, media often picks the most miserable and negative or \"click-bait\" examples of fan behaviour, as it creates the most attention and keeps the viewing figures high [@arouh_toxic_2020], [@proctor_editors_2018]. These circumstances are additionally amplified by social media plattforms, as they promote toxic behaviour, because it usually creates a lot of interactions. Therefore, it is our overall understanding of what a fan is that marks a him as a toxic \"other\".\n",
    "\n",
    "What is also observed, is that \"toxic\" fans often fall back to racist and mysogenistic behaviour compared with hate speech in order to defend their fan object or view point. This often comes with a feeling of \"power loss\" for the \"toxic fan\". Because of that, current social-, ideological- and political conflicts are becoming more and more frequent as a topic in toxic behaviour [@proctor_editors_2018], [@arouh_toxic_2020], [@toxic_fandom]. For some members of the fan communities, this feeling of power loss is amplified by current political circumstances where they feel a feeling of disempowerment at their loss of priviliged status in society because of gender discussions or woman rights movements. Thus toxic fans are often painted as angry white, heterosexual men or members of the \"alt-right\" community. However in many cases, fan communities are used as a plattform to spread this hatered or ideological ideas because it creates a lot of attention in social networks as well as from the media. The media then progresses to paint fandom and online culture as more and more toxic because it creates \"maximum cultural penetration\" [@proctor_editors_2018]. This trend has led to the phenomenon of *progressive toxicity*, where other fans \"rush to prove one's moral superiority by speaking out against some racist, sexist or otherwise hurtful sentiment, the sentiment is often amplified on a scale that wouldn't have been possible had people not taken the bait\" [@proctor_editors_2018]. This rush to prove morally better than the toxic other often leads to toxic behavior by the defender itself. Because of that, toxic practices more and more frequently are instantiations of larger political or cultural polarizations and they depict the current socio-political climate. Thus toxic fan behaviour is often observed as a conlflict between the \"political correct\" pro-diversity crowd, which are also called social justice warriors (SJWs) and the members of the so-called \"alt-right\" hell-bent [@proctor_editors_2018].\n",
    "\n",
    "However toxic fan behaviour is not limited to racist, misogynistic comments that can also include hate-speech. Some toxic fan are even going as far as to writing death or rape threats, doxing people (doxing refers to leaking personal information online) or to show abusive and harassing behaviour in public against other groups [@proctor_editors_2018], [@arouh_toxic_2020]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b974c295",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43eecf97",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The dataset that will be used throughout this thesis consists of 40200 Comments with replys from 500 youtube videos that were uploaded since 2020 of the formula 1 youtube channel. To obtain this data, the Youtube API V3 was used.\n",
    "\n",
    "First up, the API has to be initialised, for this an api key is needed, that has to be stored in a .env file in the same directory as the jupyter notebook. This api key is then read in the following code cell and the youtube api is initialized through googles official googleapiclient library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40637624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_keys = dotenv_values(\"keys.env\")\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "api_key = api_keys[\"YOUTUBE_API_KEY\"]\n",
    "max_results = 1000\n",
    "youtube_api = googleapiclient.discovery.build(api_service_name, api_version, developerKey = api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c16df0b",
   "metadata": {},
   "source": [
    "Now request to the Youtube API V3 can be made. Before we can scrape comments, the video id of the video that comments want to be obtain from is needed. Therefore, data about all videos since 2020 until now are requested. However the api will only retrieve 50 items per request, if there are more items that fit the search query the response is paged and contains a *nextPageToken*, that can be used to obtain the next 50 items. Requesting all videos since 2020 allows the dataset to span a timeframe of three years and will allow to analyze toxicity over time as well and will also paint a broader picture of how the F1 fandom developed. After obtaining all video information, the video ids are extracted and safed into a list, which is used later to obtain the actual comment threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Formula1_official_channel = youtube_api.channels().list(part='snippet' ,forUsername='Formula1').execute()['items'][0]\n",
    "videos_after_2020 = youtube_api.search().list(channelId=Formula1_official_channel[\"id\"],\n",
    "        maxResults=max_results,\n",
    "        publishedAfter=\"2020-01-01T00:00:00Z\",\n",
    "        part='id').execute()\n",
    "video_ids_after_2020 = [item['id']['videoId'] for item in videos_after_2020['items']]\n",
    "while len(video_ids_after_2020) < max_results and \"nextPageToken\" in videos_after_2020.keys():\n",
    "        videos_after_2020 = youtube_api.search().list(channelId=Formula1_official_channel[\"id\"],\n",
    "        maxResults=max_results,\n",
    "        publishedAfter=\"2020-01-01T00:00:00Z\",\n",
    "        part='id',\n",
    "        pageToken=videos_after_2020[\"nextPageToken\"]).execute()\n",
    "        video_ids_after_2020 = video_ids_after_2020 + [item['id']['videoId'] for item in videos_after_2020['items']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a71dd7a1",
   "metadata": {},
   "source": [
    "Besides the list of video ids, the data is also parsed into a dataframe. This allows to take general video information such as like count, video title, the overall comment count etc. into consideration for the final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for video_id in video_ids_after_2020:\n",
    "    video_data = youtube_api.videos().list(part='snippet, statistics', id=video_id).execute()\n",
    "    snippet = video_data['items'][0]['snippet']\n",
    "    statistics = video_data['items'][0]['statistics']\n",
    "    df_list.append(\n",
    "    {\n",
    "        \"video_id\":video_id,\n",
    "        \"title\": snippet['title'],\n",
    "        \"description\": snippet['description'],\n",
    "        \"channel\": snippet['channelTitle'],\n",
    "        \"published_at\": snippet['publishedAt'],\n",
    "        \"tags\": snippet['tags'] if \"tags\" in snippet.keys() else None,\n",
    "        \"like_count\": statistics['likeCount'],\n",
    "        \"favorite_count\": statistics['favoriteCount'],\n",
    "        \"comment_count\": statistics['commentCount'] if \"commentCount\" in statistics.keys() else 0\n",
    "    })\n",
    "\n",
    "videos = pd.DataFrame(df_list)\n",
    "videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988c4e58",
   "metadata": {},
   "source": [
    "Now that all the necessary video information has been obtained, the actual comments and replys can be requested. In order to achieve this, for every video id that has been retrieved earlier, a list of 15 comment threads is requested. Every comment thread consists of a topcomment, that has a number of replys associated with it. Because of the maximum quota of 10000 request units per day, for each video only 15 comments can be obtained, as each comment request costs one unit, for all 500 videos for 15 commenthreads per video, a quota usage of 7500 applies. Now for each retrieved top comment a maximum of 10 replies are requested. The corresponding data, is then parsed into one large dataframe, that contains the comment text as well as administrative information like the video id as well as the comment id and further useful information like the number of likes a comment / reply has or the publishing date. This additional information allows to further reason about the amount of interaction the particular comment got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_comments = []\n",
    "for video_id in video_ids_after_2020:\n",
    "    if videos.loc[videos['video_id'] == video_id].comment_count.iloc[0] == 0:\n",
    "        continue\n",
    "    top_level_comments = youtube_api.commentThreads().list(part=\"snippet\",\n",
    "        maxResults=15,\n",
    "        order=\"relevance\",\n",
    "        videoId=video_id).execute()['items']\n",
    "    for top_level_comment in top_level_comments:\n",
    "        replies = youtube_api.comments().list(part=\"snippet\",\n",
    "            maxResults=10,\n",
    "            parentId=top_level_comment['snippet']['topLevelComment']['id']).execute()['items']\n",
    "        df_list_comments.append(\n",
    "        {\n",
    "            \"video_id\": video_id,\n",
    "            \"id\": top_level_comment['snippet']['topLevelComment']['id'],\n",
    "            \"text\": top_level_comment['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "            \"user\": top_level_comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "            \"like_count\": top_level_comment['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "            \"published_at\": top_level_comment['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "            \"reply_count\": top_level_comment['snippet']['totalReplyCount']\n",
    "        })\n",
    "        for reply in replies:\n",
    "            df_list_comments.append(\n",
    "            {\n",
    "                \"video_id\": video_id,\n",
    "                \"id\": reply['id'],\n",
    "                \"text\": reply['snippet']['textDisplay'],\n",
    "                \"user\": reply['snippet']['authorChannelId']['value'],\n",
    "                \"like_count\": reply['snippet']['likeCount'],\n",
    "                \"published_at\": reply['snippet']['publishedAt'],\n",
    "                \"reply_count\": 0\n",
    "            })\n",
    "\n",
    "comment_df: pd.DataFrame = pd.DataFrame(df_list_comments)\n",
    "comment_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9830cb35",
   "metadata": {},
   "source": [
    "Last but not least the dataset is saved into a \"pickle\" file, which allows efficient storage of dataframes. This is especially useful if the notebook has to be restarted because the dataset doesn't has to be build from scratch and no quota or api access is required to perform analysis on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.to_pickle(\"datasets/video_data.pkl\")\n",
    "comment_df.to_pickle(\"datasets/comment_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos: pd.DataFrame = pd.read_pickle(\"datasets/video_data.pkl\")\n",
    "comment_df: pd.DataFrame = pd.read_pickle(\"datasets/comment_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba4dbb67",
   "metadata": {},
   "source": [
    "### Dataset limitations\n",
    "Because of the quoate limit google has set for the youtube api, the dataset is only depicting a small section of the actual circumstances in the Formula 1 fandom. For example, for one video, a maximum of $15*10 = 150$ comments will be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.comment_count = videos.comment_count.astype(int)\n",
    "videos.comment_count.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7613297e",
   "metadata": {},
   "source": [
    "However, on average a video has 1250 comments. Thus a lot of fan interaction will be missed and is not included in this dataset. In addition to that, the dataset only uses the Youtube API as a source, however Formula 1 fandom spans over multiple platforms, especially Twitter, Instagram and Reddit. Thus it is possible that depending on the plattform toxic user interactions may be more frequent as they are governed differently. Nevertheless the dataset spans over a total of 40200 comments that can be analysed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8a094b",
   "metadata": {},
   "source": [
    "## Dictionary Analysis\n",
    "\n",
    "### Othrus Lexicon for Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e76030",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d12e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dictionaries/toxic_words.txt\") as toxic_words_file:\n",
    "    set_of_toxic_words: set = set([word.strip() for word in toxic_words_file.readlines()])\n",
    "set_of_toxic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9291c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def dictionary_analysis_over_set_intersection(dict_name: str, dict_set: set, data: pd.DataFrame) -> Tuple[pd.DataFrame, Counter]:\n",
    "    dict_word_counter: Counter = Counter()\n",
    "    dict_word_count: list = []\n",
    "    for row in data.text:\n",
    "        dict_words_in_comment: set = set(word_tokenize(row)).intersection(dict_set)\n",
    "        dict_word_counter.update(dict_words_in_comment)\n",
    "        dict_word_count.append(len(dict_words_in_comment))\n",
    "    data[f\"{dict_name}_word_count\"] = dict_word_count\n",
    "    return data, dict_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df, toxic_word_counter = dictionary_analysis_over_set_intersection(dict_name=\"toxic\", dict_set=set_of_toxic_words, data=comment_df)\n",
    "comment_df.loc[comment_df[\"toxic_word_count\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_word_counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dac9531d",
   "metadata": {},
   "source": [
    "### Grievance Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk import TreebankWordDetokenizer\n",
    "import swifter\n",
    "\n",
    "stemmer: PorterStemmer = PorterStemmer()\n",
    "detokenizer: TreebankWordDetokenizer = TreebankWordDetokenizer()\n",
    "stemmed_comments: pd.DataFrame = comment_df.copy()\n",
    "stemmed_comments[\"text\"] = stemmed_comments.text.swifter.apply(lambda text: detokenizer.detokenize([stemmer.stem(word) for word in word_tokenize(text)]))\n",
    "stemmed_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141973b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "grievance_dict_df = pd.read_csv(\"dictionaries/grievancedictionary/dictionary_versions/dictionary_5plus.csv\")\n",
    "grievance_dict_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "categorys = grievance_dict_df.category.unique()\n",
    "categorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40dd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "grievance_set_dictionary: Dict[str, Counter] = defaultdict(Counter)\n",
    "for category in categorys:\n",
    "    curr_category_set = set(grievance_dict_df.loc[grievance_dict_df.category == category].word.to_list())\n",
    "    stemmed_comments, grievance_set_dictionary[category] = dictionary_analysis_over_set_intersection(dict_name=category, dict_set=curr_category_set, data=stemmed_comments)\n",
    "stemmed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa91c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.merge(comment_df, stemmed_comments[[\"id\", \"deadline_word_count\", \"desperation_word_count\", \"fixation_word_count\", 'frustration_word_count', 'god_word_count',\n",
    "       'grievance_word_count', 'hate_word_count', 'help_word_count', 'honour_word_count', 'impostor_word_count', 'jealousy_word_count',\n",
    "       'loneliness_word_count', 'murder_word_count', 'paranoia_word_count', 'planning_word_count', 'relationship_word_count',\n",
    "       'soldier_word_count', 'suicide_word_count', 'surveillance_word_count', 'threat_word_count', 'violence_word_count',\n",
    "       'weaponry_word_count']], on=\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae11a84a",
   "metadata": {},
   "source": [
    "### Ethnic Slurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os.path\n",
    "\n",
    "dict_files: list = list(filter(lambda f: f[-4:] == \".csv\" ,listdir(\"dictionaries/ethnic_slurs/\")))\n",
    "dict_df: pd.DataFrame = pd.DataFrame()\n",
    "for file in dict_files:\n",
    "    part = pd.read_csv(os.path.join(\"dictionaries/ethnic_slurs\", file))\n",
    "    dict_df = pd.concat([part, dict_df])\n",
    "dict_df.reset_index(inplace=True, drop=True)\n",
    "ethnic_slurs_set: set = set(dict_df.Term.to_list())\n",
    "dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e425e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df, ethnic_slurs_counter = dictionary_analysis_over_set_intersection(dict_name=\"ethnic_slurs\", dict_set=ethnic_slurs_set, data=comment_df)\n",
    "comment_df.loc[comment_df[\"ethnic_slurs_word_count\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c931cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic_slurs_counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beccab78",
   "metadata": {},
   "source": [
    "## Transformer Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3535ee24",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a8f994c",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb13c4",
   "metadata": {},
   "source": [
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed7f3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook final.ipynb to markdown\n",
      "[NbConvertApp] Writing 25079 bytes to final.md\n",
      "Error producing PDF.\n",
      "! Undefined control sequence.\n",
      "l.130 compiled with \\LATEX\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11008"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"jupyter nbconvert --to markdown final.ipynb\")\n",
    "os.system(\"pandoc -s final.md -t pdf -o final.pdf --citeproc --bibliography=refs.bib --csl=apa.csl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641d154",
   "metadata": {},
   "source": [
    "-->"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "13409951/AYIUUYSI": {
      "DOI": "10.3758/s13428-021-01536-2",
      "URL": "https://link.springer.com/10.3758/s13428-021-01536-2",
      "abstract": "This paper introduces the Grievance Dictionary, a psycholinguistic dictionary that can be used to automatically understand language use in the context of grievance-fueled violence threat assessment. We describe the development of the dictionary, which was informed by suggestions from experienced threat assessment practitioners. These suggestions and subsequent human and computational word list generation resulted in a dictionary of 20,502 words annotated by 2318 participants. The dictionary was validated by applying it to texts written by violent and non-violent individuals, showing strong evidence for a difference between populations in several dictionary categories. Further classification tasks showed promising performance, but future improvements are still needed. Finally, we provide instructions and suggestions for the use of the Grievance Dictionary by security professionals and (violence) researchers.",
      "accessed": {
       "date-parts": [
        [
         2023,
         1,
         7
        ]
       ]
      },
      "author": [
       {
        "family": "van der Vegt",
        "given": "Isabelle"
       },
       {
        "family": "Mozes",
        "given": "Maximilian"
       },
       {
        "family": "Kleinberg",
        "given": "Bennett"
       },
       {
        "family": "Gill",
        "given": "Paul"
       }
      ],
      "container-title": "Behavior Research Methods",
      "id": "13409951/AYIUUYSI",
      "issue": "5",
      "issued": {
       "date-parts": [
        [
         "2021"
        ]
       ]
      },
      "journalAbbreviation": "Behav Res",
      "language": "en",
      "page": "2105-2119",
      "shortTitle": "The Grievance Dictionary",
      "system_id": "zotero|13409951/AYIUUYSI",
      "title": "The Grievance Dictionary: Understanding threatening language use",
      "type": "article-journal",
      "volume": "53"
     },
     "13409951/K57UGHVY": {
      "URL": "https://github.com/Orthrus-Lexicon/Toxic",
      "abstract": "A dictionary of toxic words",
      "accessed": {
       "date-parts": [
        [
         2023,
         1,
         7
        ]
       ]
      },
      "author": [
       {
        "family": "Orthrus-Lexicon",
        "given": ""
       }
      ],
      "id": "13409951/K57UGHVY",
      "issued": {
       "date-parts": [
        [
         2022,
         9,
         9
        ]
       ]
      },
      "note": "original-date: 2021-02-14T15:47:49Z",
      "system_id": "zotero|13409951/K57UGHVY",
      "title": "Orthrus Toxic Dictionary implementation",
      "type": "book"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('CAC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9841eda7af3f17386775c2d69f63acccb0fbca7caaddff7551f32402eb39582"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
